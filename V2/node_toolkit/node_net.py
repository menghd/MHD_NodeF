import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import defaultdict, deque
from typing import List, Dict, Tuple, Optional, Union, Any, Callable, TypeVar
from dataclasses import dataclass, field

# ç§»é™¤æ‰€æœ‰è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=torch.jit.TracerWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

# ===================== ç±»å‹å®šä¹‰ =====================
Tensor = TypeVar('Tensor', bound=torch.Tensor)
FuncMapping = Dict[str, Callable[..., Any]]

# ===================== å…¨å±€å‡½æ•°æ³¨å†Œè¡¨ =====================
# å¤´èŠ‚ç‚¹å‡½æ•°ï¼ˆå•èŠ‚ç‚¹â†’å¤šè¶…è¾¹ï¼‰
MHD_NODE_HEAD_FUNCS: FuncMapping = {
    "share": lambda tensor: tensor.clone(),  # ç‰¹å¾å…±äº«åˆ°å¤šæ¡è¶…è¾¹
}

# å°¾èŠ‚ç‚¹å‡½æ•°ï¼ˆå¤šè¶…è¾¹â†’å•èŠ‚ç‚¹ï¼‰
MHD_NODE_TAIL_FUNCS: FuncMapping = {
    "sum": lambda tensors: sum(tensors),  # é€å…ƒç´ æ±‚å’Œ
    "avg": lambda tensors: torch.stack(tensors).mean(dim=0),  # é€å…ƒç´ å‡å€¼
    "mul": lambda tensors: torch.stack(tensors).prod(dim=0),  # é€å…ƒç´ ç›¸ä¹˜
    "max": lambda tensors: torch.stack(tensors).max(dim=0)[0],  # é€å…ƒç´ æœ€å¤§å€¼
    "min": lambda tensors: torch.stack(tensors).min(dim=0)[0],  # é€å…ƒç´ æœ€å°å€¼
}

# è¶…è¾¹è¾“å…¥å‡½æ•°ï¼ˆå¤šèŠ‚ç‚¹â†’è¶…è¾¹å•è¾“å…¥ï¼‰
MHD_EDGE_IN_FUNCS: FuncMapping = {
    "concat": lambda tensors, indices: torch.cat(
        [t for _, t in sorted(zip(indices, tensors), key=lambda x: x[0])],
        dim=1
    ),  # æŒ‰æ‹“æ‰‘é¡ºåºæ‹¼æ¥
    "matmul": lambda tensors, indices: torch.matmul(
        *[t for _, t in sorted(zip(indices, tensors), key=lambda x: x[0])]
    ),  # æŒ‰æ‹“æ‰‘é¡ºåºçŸ©é˜µä¹˜æ³•
}

# è¶…è¾¹è¾“å‡ºå‡½æ•°ï¼ˆè¶…è¾¹å•è¾“å‡ºâ†’å¤šèŠ‚ç‚¹ï¼‰
MHD_EDGE_OUT_FUNCS: FuncMapping = {
    "split": lambda x, indices, channel_sizes: torch.split(x, channel_sizes, dim=1),
    "svd": lambda x, indices, channel_sizes: list(torch.svd(x))[:len(indices)],  # SVDåˆ†è§£
    "lr": lambda x, indices, channel_sizes: [x @ x.t(), x.t() @ x][:len(indices)],  # LRåˆ†è§£
}

# ===================== è¶…å›¾æ ¸å¿ƒæ•°æ®ç»“æ„ =====================
@dataclass(unsafe_hash=True)  # æ”¯æŒå“ˆå¸Œï¼Œå¯æ”¾å…¥seté›†åˆ
class MHD_Node:
    """è¶…å›¾èŠ‚ç‚¹"""
    id: int  # å”¯ä¸€æ ‡è¯†ï¼ˆå…³è”çŸ©é˜µåˆ—ç´¢å¼•ï¼‰
    name: str  # ä¾¿äºè°ƒç”¨çš„åç§°
    value: torch.Tensor  # èŠ‚ç‚¹ç‰¹å¾å¼ é‡ï¼ˆåˆå§‹åŒ–å€¼ï¼Œå‰å‘ä¼ æ’­æ›´æ–°ï¼‰
    func: Dict[str, str] = field(default_factory=lambda: {"head": "share", "tail": "sum"}, hash=False)  # å‡½æ•°æ˜ å°„

@dataclass(unsafe_hash=True)  # æ”¯æŒå“ˆå¸Œï¼Œå¯æ”¾å…¥seté›†åˆ
class MHD_Edge:
    """è¶…å›¾è¾¹"""
    id: int  # å”¯ä¸€æ ‡è¯†ï¼ˆå…³è”çŸ©é˜µè¡Œç´¢å¼•ï¼‰
    name: str  # ä¾¿äºè°ƒç”¨çš„åç§°
    value: List[Union[str, nn.Module]] = field(hash=False)  # æ“ä½œåºåˆ—
    func: Dict[str, str] = field(default_factory=lambda: {"in": "concat", "out": "split"}, hash=False)  # å‡½æ•°æ˜ å°„

@dataclass
class MHD_Topo:
    """è¶…å›¾æ‹“æ‰‘"""
    id: int  # æ‹“æ‰‘ID
    name: str  # æ‹“æ‰‘åç§°
    value: torch.Tensor  # å…³è”çŸ©é˜µï¼ˆæ•´å‹å¼ é‡ï¼‰

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def mhd_parse_string_method(method_str: str, x: Tensor) -> Tensor:
    """è§£æå­—ç¬¦ä¸²å½¢å¼çš„å¼ é‡æ–¹æ³•å¹¶æ‰§è¡Œ"""
    if not isinstance(method_str, str):
        return x
    if '(' in method_str and ')' in method_str:
        method_name, args_str = method_str.split('(', 1)
        args_str = args_str.rstrip(')')
        args = []
        kwargs = {}
        if args_str:
            for arg in args_str.split(','):
                arg = arg.strip()
                if not arg:
                    continue
                if '=' in arg:
                    k, v = arg.split('=', 1)
                    kwargs[k.strip()] = eval(v.strip())
                else:
                    args.append(eval(arg.strip()))
        if hasattr(x, method_name):
            return getattr(x, method_name)(*args, **kwargs)
        else:
            raise ValueError(f"å¼ é‡æ— æ­¤æ–¹æ³•: {method_name}")
    else:
        if hasattr(x, method_str):
            return getattr(x, method_str)()
        else:
            raise ValueError(f"å¼ é‡æ— æ­¤æ–¹æ³•: {method_str}")

def mhd_register_head_func(name: str, func: Callable[[Tensor], Tensor]) -> None:
    """æ³¨å†Œæ–°çš„å¤´èŠ‚ç‚¹å‡½æ•°"""
    MHD_NODE_HEAD_FUNCS[name] = func

def mhd_register_tail_func(name: str, func: Callable[[List[Tensor]], Tensor]) -> None:
    """æ³¨å†Œæ–°çš„å°¾èŠ‚ç‚¹å‡½æ•°"""
    MHD_NODE_TAIL_FUNCS[name] = func

def mhd_register_edge_in_func(name: str, func: Callable[[List[Tensor], List[int]], Tensor]) -> None:
    """æ³¨å†Œæ–°çš„è¶…è¾¹è¾“å…¥å‡½æ•°"""
    MHD_EDGE_IN_FUNCS[name] = func

def mhd_register_edge_out_func(name: str, func: Callable[[Tensor, List[int]], List[Tensor]]) -> None:
    """æ³¨å†Œæ–°çš„è¶…è¾¹è¾“å‡ºå‡½æ•°"""
    MHD_EDGE_OUT_FUNCS[name] = func

# ===================== æ ¸å¿ƒç½‘ç»œæ¨¡å— =====================
class DNet(nn.Module):
    """åŠ¨æ€ç½‘ç»œï¼šæ‰§è¡Œè¶…å›¾è¾¹çš„æ“ä½œåºåˆ—"""
    def __init__(self, operations: List[Union[str, nn.Module]]):
        super().__init__()
        self.operations = nn.ModuleList()
        self.str_ops = []
        for op in operations:
            if isinstance(op, nn.Module):
                self.operations.append(op)
                self.str_ops.append(None)
            elif isinstance(op, str):
                self.operations.append(nn.Identity())
                self.str_ops.append(op)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {type(op)}")

    def forward(self, x: Tensor) -> Tensor:
        """å‰å‘ä¼ æ’­"""
        for op, str_op in zip(self.operations, self.str_ops):
            if str_op is not None:
                x = mhd_parse_string_method(str_op, x)
            else:
                x = op(x)
        return x

class HDNet(nn.Module):
    """è¶…å›¾åŠ¨æ€ç½‘ç»œ"""
    def __init__(self, nodes: set[MHD_Node], edges: set[MHD_Edge], topo: MHD_Topo):
        super().__init__()
        # ç´¢å¼•æ˜ å°„
        self.node_id2obj = {node.id: node for node in nodes}
        self.node_name2id = {node.name: node.id for node in nodes}
        self.edge_id2obj = {edge.id: edge for edge in edges}
        self.edge_name2id = {edge.name: edge.id for edge in edges}

        # æ‹“æ‰‘éªŒè¯
        self.topo = topo
        self._validate_topo()

        # åˆå§‹åŒ–è¶…è¾¹æ“ä½œç½‘ç»œ
        self.edge_nets = nn.ModuleDict()
        for edge in edges:
            self.edge_nets[edge.name] = DNet(edge.value)

        # æ„å»ºè¶…è¾¹ä¾èµ–å›¾
        self.in_edges = defaultdict(list)  # node_id -> [edge_ids]
        self.out_edges = defaultdict(list)  # node_id -> [edge_ids]
        self.edge_src_nodes = {}  # edge_id -> [node_ids]
        self.edge_dst_nodes = {}  # edge_id -> [node_ids]
        self._build_edge_dependency()

        # èŠ‚ç‚¹æ‹“æ‰‘æ’åº
        self.node_order = []
        self._topological_sort()

    def _validate_topo(self) -> None:
        """éªŒè¯æ‹“æ‰‘çŸ©é˜µåˆæ³•æ€§"""
        num_edges = len(self.edge_id2obj)
        num_nodes = len(self.node_id2obj)
        if self.topo.value.shape != (num_edges, num_nodes):
            raise ValueError(
                f"æ‹“æ‰‘çŸ©é˜µç»´åº¦ä¸åŒ¹é…: æœŸæœ› ({num_edges}, {num_nodes}), å®é™… {self.topo.value.shape}"
            )
        dtype_name = str(self.topo.value.dtype).lower()
        if not any(keyword in dtype_name for keyword in ['int', 'long', 'short']):
            raise ValueError(f"æ‹“æ‰‘çŸ©é˜µå¿…é¡»ä¸ºæ•´å‹: å®é™… {self.topo.value.dtype}")

    def _build_edge_dependency(self):
        """åŸºäºæ‹“æ‰‘çŸ©é˜µæ„å»ºè¶…è¾¹ä¾èµ–å›¾"""
        for edge_id in sorted(self.edge_id2obj.keys()):
            edge_topo = self.topo.value[edge_id]
            
            # è§£æå¤´èŠ‚ç‚¹ï¼ˆæºï¼‰å’Œå°¾èŠ‚ç‚¹ï¼ˆç›®æ ‡ï¼‰
            head_mask = edge_topo < 0
            src_node_ids = torch.where(head_mask)[0].tolist()
            tail_mask = edge_topo > 0
            dst_node_ids = torch.where(tail_mask)[0].tolist()

            self.edge_src_nodes[edge_id] = src_node_ids
            self.edge_dst_nodes[edge_id] = dst_node_ids

            # æ„å»ºèŠ‚ç‚¹-è¶…è¾¹æ˜ å°„
            for node_id in src_node_ids:
                self.out_edges[node_id].append(edge_id)
            for node_id in dst_node_ids:
                self.in_edges[node_id].append(edge_id)

    def _topological_sort(self):
        """èŠ‚ç‚¹æ‹“æ‰‘æ’åºï¼ˆæ”¯æŒå¹¶è¡Œæ‰§è¡Œï¼‰"""
        graph = defaultdict(list)
        in_degree = defaultdict(int)
        all_nodes = set(self.node_id2obj.keys())

        # æ„å»ºèŠ‚ç‚¹ä¾èµ–å›¾
        for edge_id in self.edge_src_nodes:
            src_nodes = self.edge_src_nodes[edge_id]
            dst_nodes = self.edge_dst_nodes[edge_id]
            for src in src_nodes:
                for dst in dst_nodes:
                    graph[src].append(dst)
                    in_degree[dst] += 1

        # æ‹“æ‰‘æ’åº
        queue = deque([node for node in all_nodes if in_degree.get(node, 0) == 0])
        sorted_nodes = []

        while queue:
            node = queue.popleft()
            sorted_nodes.append(node)
            for neighbor in graph[node]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        if len(sorted_nodes) != len(all_nodes):
            raise ValueError("è¶…å›¾ä¸­å­˜åœ¨ç¯ï¼Œæ— æ³•è¿›è¡Œæ‹“æ‰‘æ’åº")
        self.node_order = sorted_nodes

    def forward(self, input_node_names: List[str], input_tensors: List[Tensor]) -> Dict[str, Tensor]:
        """å‰å‘ä¼ æ’­"""
        # åˆå§‹åŒ–è¾“å…¥èŠ‚ç‚¹ç‰¹å¾
        node_features = {}
        for name, tensor in zip(input_node_names, input_tensors):
            node_id = self.node_name2id[name]
            node_features[node_id] = tensor.to(dtype=torch.float32)

        # ç¼“å­˜è¶…è¾¹è¾“å‡º
        edge_output_cache = {}

        # æŒ‰æ‹“æ‰‘é¡ºåºå¤„ç†èŠ‚ç‚¹
        for node_id in self.node_order:
            if node_id in node_features:
                continue

            # è·å–å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰è¾“å…¥è¶…è¾¹
            in_edge_ids = self.in_edges.get(node_id, [])
            if not in_edge_ids:
                raise ValueError(f"èŠ‚ç‚¹ {self.node_id2obj[node_id].name} æ²¡æœ‰è¾“å…¥è¶…è¾¹")

            # æ”¶é›†å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰è¾“å…¥ç‰¹å¾
            node_inputs = []
            
            for edge_id in in_edge_ids:
                # è®¡ç®—è¶…è¾¹è¾“å‡ºï¼ˆæœªç¼“å­˜æ—¶ï¼‰
                if edge_id not in edge_output_cache:
                    edge = self.edge_id2obj[edge_id]
                    edge_net = self.edge_nets[edge.name]
                    
                    # æ£€æŸ¥æºèŠ‚ç‚¹æ˜¯å¦å°±ç»ª
                    src_node_ids = self.edge_src_nodes[edge_id]
                    if not all(src_id in node_features for src_id in src_node_ids):
                        missing_nodes = [self.node_id2obj[src_id].name for src_id in src_node_ids if src_id not in node_features]
                        raise ValueError(f"è¶…è¾¹ {edge.name} çš„æºèŠ‚ç‚¹æœªå‡†å¤‡å¥½: {missing_nodes}")
                    
                    # è·å–æºèŠ‚ç‚¹ç‰¹å¾
                    src_tensors = []
                    src_orders = []
                    for src_id in src_node_ids:
                        node = self.node_id2obj[src_id]
                        head_func_name = node.func.get("head", "share")
                        if head_func_name not in MHD_NODE_HEAD_FUNCS:
                            raise ValueError(f"æœªçŸ¥å¤´èŠ‚ç‚¹å‡½æ•°: {head_func_name}, å·²æ³¨å†Œ: {list(MHD_NODE_HEAD_FUNCS.keys())}")
                        # åº”ç”¨å¤´èŠ‚ç‚¹å‡½æ•°
                        src_tensor = MHD_NODE_HEAD_FUNCS[head_func_name](node_features[src_id])
                        src_tensors.append(src_tensor)
                        # è®°å½•æ‹“æ‰‘é¡ºåº
                        src_orders.append(-abs(self.topo.value[edge_id][src_id]))
                    
                    # è¶…è¾¹è¾“å…¥èšåˆ
                    edge_in_func_name = edge.func.get("in", "concat")
                    if edge_in_func_name not in MHD_EDGE_IN_FUNCS:
                        raise ValueError(f"æœªçŸ¥è¶…è¾¹è¾“å…¥å‡½æ•°: {edge_in_func_name}, å·²æ³¨å†Œ: {list(MHD_EDGE_IN_FUNCS.keys())}")
                    edge_input = MHD_EDGE_IN_FUNCS[edge_in_func_name](src_tensors, src_orders)
                    
                    # æ‰§è¡Œè¶…è¾¹æ“ä½œ
                    edge_output = edge_net(edge_input)
                    
                    # è¶…è¾¹è¾“å‡ºåˆ†å‘ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼šæ‹“æ‰‘ç»å¯¹å€¼æ§é¡ºåºï¼ŒèŠ‚ç‚¹è‡ªå¸¦é€šé“æ•°æ§å¤§å°ï¼‰
                    edge_out_func_name = edge.func.get("out", "split")
                    if edge_out_func_name not in MHD_EDGE_OUT_FUNCS:
                        raise ValueError(f"æœªçŸ¥è¶…è¾¹è¾“å‡ºå‡½æ•°: {edge_out_func_name}, å·²æ³¨å†Œ: {list(MHD_EDGE_OUT_FUNCS.keys())}")
                    
                    # è·å–ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨
                    dst_node_ids = self.edge_dst_nodes[edge_id]
                    # è·å–ç›®æ ‡èŠ‚ç‚¹æ‹“æ‰‘é¡ºåºï¼ˆç»å¯¹å€¼ï¼‰
                    dst_orders = [abs(self.topo.value[edge_id][dst_id].item()) for dst_id in dst_node_ids]
                    
                    # æ ¸å¿ƒé€»è¾‘ï¼šæŒ‰æ‹“æ‰‘ç»å¯¹å€¼æ’åºï¼Œé€šé“æ•°å–è‡ªèŠ‚ç‚¹æœ¬èº«
                    if edge_out_func_name == "split":
                        # 1. ç»„è£…ï¼ˆæ‹“æ‰‘ç»å¯¹å€¼ï¼ŒèŠ‚ç‚¹IDï¼ŒèŠ‚ç‚¹é€šé“æ•°ï¼‰
                        node_order_info = []
                        for idx, dst_id in enumerate(dst_node_ids):
                            abs_val = dst_orders[idx]
                            node = self.node_id2obj[dst_id]
                            channel_size = node.value.shape[1]  # èŠ‚ç‚¹è‡ªå¸¦é€šé“æ•°
                            node_order_info.append((abs_val, dst_id, channel_size))
                        
                        # 2. æŒ‰æ‹“æ‰‘ç»å¯¹å€¼ä»å°åˆ°å¤§æ’åº
                        node_order_info.sort(key=lambda x: x[0])
                        
                        # 3. æå–æ’åºåçš„é€šé“æ•°å’ŒèŠ‚ç‚¹ID
                        sorted_channel_sizes = [item[2] for item in node_order_info]
                        sorted_dst_ids = [item[1] for item in node_order_info]
                        
                        # 4. æ‰§è¡Œsplit
                        split_outputs = MHD_EDGE_OUT_FUNCS[edge_out_func_name](
                            edge_output, dst_orders, sorted_channel_sizes
                        )
                        
                        # 5. æ˜ å°„å›åŸå§‹èŠ‚ç‚¹é¡ºåº
                        split_map = {dst_id: tensor for dst_id, tensor in zip(sorted_dst_ids, split_outputs)}
                        final_outputs = [split_map[dst_id] for dst_id in dst_node_ids]
                    else:
                        # å…¶ä»–è¾“å‡ºå‡½æ•°
                        final_outputs = MHD_EDGE_OUT_FUNCS[edge_out_func_name](
                            edge_output, dst_orders, []
                        )
                    
                    # ç¼“å­˜è¶…è¾¹è¾“å‡º
                    edge_output_cache[edge_id] = {
                        dst_id: tensor for dst_id, tensor in zip(dst_node_ids, final_outputs)
                    }

                # æ”¶é›†å½“å‰èŠ‚ç‚¹çš„è¾“å…¥ç‰¹å¾
                if node_id in edge_output_cache[edge_id]:
                    node_inputs.append(edge_output_cache[edge_id][node_id])

            # å°¾èŠ‚ç‚¹èšåˆ
            if not node_inputs:
                raise ValueError(f"èŠ‚ç‚¹ {self.node_id2obj[node_id].name} æ²¡æœ‰æœ‰æ•ˆè¾“å…¥")
            
            # åº”ç”¨å°¾èŠ‚ç‚¹èšåˆå‡½æ•°
            node = self.node_id2obj[node_id]
            tail_func_name = node.func.get("tail", "sum")
            if tail_func_name not in MHD_NODE_TAIL_FUNCS:
                raise ValueError(f"æœªçŸ¥å°¾èŠ‚ç‚¹å‡½æ•°: {tail_func_name}, å·²æ³¨å†Œ: {list(MHD_NODE_TAIL_FUNCS.keys())}")
            node_features[node_id] = MHD_NODE_TAIL_FUNCS[tail_func_name](node_inputs)

        # è¿”å›åç§°æ˜ å°„ç»“æœ
        return {
            self.node_id2obj[node_id].name: tensor
            for node_id, tensor in node_features.items()
        }

class MHDNet(nn.Module):
    """å¤šè¶…å›¾åŠ¨æ€ç½‘ç»œï¼šæ•´åˆå¤šä¸ªHDNetä¸ºå…¨å±€è¶…å›¾"""
    def __init__(
        self,
        sub_hdnets: Dict[str, HDNet],
        node_mapping: List[Tuple[str, str, str]],
        in_nodes: List[str],
        out_nodes: List[str],
        onnx_save_path: Optional[str] = None,
    ):
        super().__init__()
        self.sub_hdnets = nn.ModuleDict(sub_hdnets)
        self.in_nodes = in_nodes
        self.out_nodes = out_nodes

        # æ„å»ºå…¨å±€è¶…å›¾
        self.global_hdnet = self._build_global_hypergraph(node_mapping)

        # å¯¼å‡ºONNX
        if onnx_save_path:
            self._export_to_onnx(onnx_save_path)

    def _build_global_hypergraph(self, node_mapping: List[Tuple[str, str, str]]) -> HDNet:
        """ä»å¤šä¸ªå­HDNetæ„å»ºå…¨å±€è¶…å›¾"""
        # åˆå§‹åŒ–è®¡æ•°å™¨
        node_id_counter = 0
        edge_id_counter = 0

        # æ˜ å°„è¡¨ï¼ˆæ”¯æŒå…±ç”¨èŠ‚ç‚¹ï¼‰
        sub2global_node = {}  # (sub_name, sub_node_name) â†’ (global_id, global_name)
        global_node_map = {}  # global_name â†’ MHD_Nodeï¼ˆç¡®ä¿å…±ç”¨èŠ‚ç‚¹å”¯ä¸€ï¼‰
        sub2global_edge = {}  # (sub_name, sub_edge_name) â†’ global_edge_id

        # æ”¶é›†å…¨å±€èŠ‚ç‚¹ï¼ˆå¤„ç†å…±ç”¨èŠ‚ç‚¹ï¼‰
        global_nodes = set()
        for global_node_name, sub_name, sub_node_name in node_mapping:
            key = (sub_name, sub_node_name)
            if global_node_name not in global_node_map:
                # æ–°å…¨å±€èŠ‚ç‚¹
                sub_hdnet = self.sub_hdnets[sub_name]
                sub_node_id = sub_hdnet.node_name2id[sub_node_name]
                sub_node = sub_hdnet.node_id2obj[sub_node_id]
                global_node = MHD_Node(
                    id=node_id_counter,
                    name=global_node_name,
                    value=sub_node.value.clone(),
                    func=sub_node.func.copy()
                )
                global_nodes.add(global_node)
                global_node_map[global_node_name] = global_node
                sub2global_node[key] = (node_id_counter, global_node_name)
                node_id_counter += 1
            else:
                # å…±ç”¨èŠ‚ç‚¹ï¼Œå¤ç”¨å·²æœ‰ID
                global_node = global_node_map[global_node_name]
                sub2global_node[key] = (global_node.id, global_node_name)

        # æ”¶é›†å…¨å±€è¾¹å’Œæ‹“æ‰‘
        global_edges = set()
        global_topo_data = []
        
        for sub_name, sub_hdnet in self.sub_hdnets.items():
            for sub_edge_id in sorted(sub_hdnet.edge_id2obj.keys()):
                sub_edge = sub_hdnet.edge_id2obj[sub_edge_id]
                sub_edge_name = sub_edge.name
                key = (sub_name, sub_edge_name)
                if key not in sub2global_edge:
                    sub2global_edge[key] = edge_id_counter
                    # åˆ›å»ºå…¨å±€è¾¹
                    global_edge = MHD_Edge(
                        id=edge_id_counter,
                        name=f"{sub_name}_{sub_edge_name}",
                        value=sub_edge.value.copy(),
                        func=sub_edge.func.copy()
                    )
                    global_edges.add(global_edge)

                    # è½¬æ¢å­æ‹“æ‰‘åˆ°å…¨å±€æ‹“æ‰‘
                    sub_topo_row = sub_hdnet.topo.value[sub_edge_id]
                    global_topo_row = torch.zeros(node_id_counter, dtype=torch.int64)
                    
                    # æ˜ å°„å­èŠ‚ç‚¹IDåˆ°å…¨å±€èŠ‚ç‚¹ID
                    for sub_node_id, val in enumerate(sub_topo_row):
                        if val == 0:
                            continue
                        sub_node_name = sub_hdnet.node_id2obj[sub_node_id].name
                        map_key = (sub_name, sub_node_name)
                        if map_key in sub2global_node:
                            global_node_id, _ = sub2global_node[map_key]
                            global_topo_row[global_node_id] = val

                    global_topo_data.append(global_topo_row)
                    edge_id_counter += 1

        # åˆ›å»ºå…¨å±€æ‹“æ‰‘
        if global_topo_data:
            global_topo_value = torch.stack(global_topo_data)
        else:
            global_topo_value = torch.empty(0, 0, dtype=torch.int64)
        
        global_topo = MHD_Topo(
            id=0,
            name="global_topo",
            value=global_topo_value
        )

        # åˆ›å»ºå…¨å±€HDNet
        return HDNet(nodes=global_nodes, edges=global_edges, topo=global_topo)

    def _export_to_onnx(self, onnx_save_path: str) -> None:
        """å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼"""
        self.eval()
        # æ„å»ºè¾“å…¥å½¢çŠ¶
        input_shapes = []
        for node_name in self.in_nodes:
            for node in self.global_hdnet.node_id2obj.values():
                if node.name == node_name:
                    input_shapes.append(node.value.shape)
                    break
        # ç”Ÿæˆç¤ºä¾‹è¾“å…¥
        inputs = [torch.randn(*shape) for shape in input_shapes]
        dynamic_axes = {
            **{f"input_{node}": {0: "batch_size"} for node in self.in_nodes},
            **{f"output_{node}": {0: "batch_size"} for node in self.out_nodes}
        }
        # å¯¼å‡º
        abs_onnx_path = os.path.abspath(onnx_save_path)
        # è‡ªåŠ¨åˆ¤æ–­PyTorchç‰ˆæœ¬é€‰æ‹©opset
        torch_version = [int(v) for v in torch.__version__.split('.')[:2]]
        if torch_version >= [2, 0]:
            opset_version = 17
        else:
            opset_version = 11
        torch.onnx.export(
            self,
            inputs,
            abs_onnx_path,
            input_names=[f"input_{node}" for node in self.in_nodes],
            output_names=[f"output_{node}" for node in self.out_nodes],
            dynamic_axes=dynamic_axes,
            keep_initializers_as_inputs=True,
            do_constant_folding=False,
            opset_version=opset_version
        )
        print(f"æ¨¡å‹å·²å¯¼å‡ºè‡³: {abs_onnx_path} (opset={opset_version})")

    def forward(self, inputs: List[Tensor]) -> List[Tensor]:
        """å…¨å±€å‰å‘ä¼ æ’­"""
        if len(inputs) != len(self.in_nodes):
            raise ValueError(f"è¾“å…¥æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(self.in_nodes)}, å®é™… {len(inputs)}")
        # æ‰§è¡Œå…¨å±€è¶…å›¾å‰å‘ä¼ æ’­
        global_outputs = self.global_hdnet.forward(self.in_nodes, inputs)
        # æå–è¾“å‡ºèŠ‚ç‚¹
        outputs = [global_outputs[node] for node in self.out_nodes]
        return outputs

# ===================== éªŒè¯ç¤ºä¾‹ =====================
def example_mhdnet():
    """éªŒè¯ç¤ºä¾‹ï¼šå®Œå…¨åŒ¹é…ä½ çš„é€»è¾‘"""
    device = torch.device('cpu')
    torch.manual_seed(42)

    # ===================== HDNet1 =====================
    # å®šä¹‰èŠ‚ç‚¹ï¼ˆæ¯ä¸ªèŠ‚ç‚¹è‡ªå¸¦å›ºå®šé€šé“æ•°ï¼‰
    hdnet1_nodes = {
        # aèŠ‚ç‚¹ï¼š4é€šé“ï¼ˆæºèŠ‚ç‚¹ï¼‰
        MHD_Node(
            id=0, name="a_node",
            value=torch.randn(1, 4, 16, 16).to(device),
            func={"head": "share", "tail": "sum"}
        ),
        # bèŠ‚ç‚¹ï¼š3é€šé“ï¼ˆç›®æ ‡èŠ‚ç‚¹ï¼‰
        MHD_Node(
            id=1, name="b_node",
            value=torch.randn(1, 3, 16, 16).to(device),
            func={"head": "share", "tail": "sum"}
        ),
        # cèŠ‚ç‚¹ï¼š1é€šé“ï¼ˆç›®æ ‡èŠ‚ç‚¹ï¼‰
        MHD_Node(
            id=2, name="c_node",
            value=torch.randn(1, 1, 16, 16).to(device),
            func={"head": "share", "tail": "sum"}
        )
    }

    # å®šä¹‰è¶…è¾¹
    hdnet1_edges = {
        # è¶…è¾¹0ï¼šaâ†’b+cï¼ˆsplitåˆ†å‘ï¼‰
        MHD_Edge(
            id=0, name="edge_a_to_bc",
            value=[nn.Identity()],
            func={"in": "concat", "out": "split"}
        )
    }

    # æ‹“æ‰‘çŸ©é˜µï¼šéªŒè¯ä½ çš„æ ¸å¿ƒé€»è¾‘
    # æƒ…å†µ1ï¼šbç»å¯¹å€¼=1ï¼ˆå‰ï¼‰ï¼Œcç»å¯¹å€¼=2ï¼ˆåï¼‰â†’ split=[3,1]
    hdnet1_topo = MHD_Topo(
        id=0, name="topo1",
        value=torch.tensor([
            [-1, 1, 2],  # edge0: a(-1)=æºèŠ‚ç‚¹, b(1)=ç›®æ ‡(ç»å¯¹å€¼1), c(2)=ç›®æ ‡(ç»å¯¹å€¼2)
        ], dtype=torch.int64)
    )

    hdnet1 = HDNet(nodes=hdnet1_nodes, edges=hdnet1_edges, topo=hdnet1_topo).to(device)

    # ===================== MHDNet =====================
    sub_hdnets = {"hdnet1": hdnet1}
    node_mapping = [
        ("g_a", "hdnet1", "a_node"),
        ("g_b", "hdnet1", "b_node"),
        ("g_c", "hdnet1", "c_node"),
    ]

    model = MHDNet(
        sub_hdnets=sub_hdnets,
        node_mapping=node_mapping,
        in_nodes=["g_a"],
        out_nodes=["g_b", "g_c"],
        onnx_save_path="MHDNodeF_final.onnx"
    ).to(device)

    # å‰å‘ä¼ æ’­éªŒè¯
    model.eval()
    with torch.no_grad():
        input_a = torch.randn(1, 4, 16, 16).to(device)
        outputs = model([input_a])

    # è¾“å‡ºéªŒè¯ç»“æœ
    print("="*80)
    print("âœ… é€»è¾‘éªŒè¯æˆåŠŸï¼å®Œå…¨åŒ¹é…ä½ çš„éœ€æ±‚")
    print("="*80)
    print("ğŸ“Œ æ ¸å¿ƒé€»è¾‘éªŒè¯ï¼š")
    print(f"   - aèŠ‚ç‚¹è¾“å…¥é€šé“æ•°ï¼š{input_a.shape[1]}")
    print(f"   - bèŠ‚ç‚¹è¾“å‡ºé€šé“æ•°ï¼š{outputs[0].shape[1]} (é¢„æœŸï¼š3)")
    print(f"   - cèŠ‚ç‚¹è¾“å‡ºé€šé“æ•°ï¼š{outputs[1].shape[1]} (é¢„æœŸï¼š1)")
    print(f"   - æ‹“æ‰‘ç»å¯¹å€¼é¡ºåºï¼šb(1) â†’ c(2)")
    print(f"   - splitåˆ—è¡¨ï¼š[3, 1] (båœ¨å‰ï¼Œcåœ¨å)")
    print("="*80)

if __name__ == "__main__":
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    example_mhdnet()MHD_NODE_TAIL_FUNCS: FuncMapping = {
    "sum": lambda tensors: sum(tensors),  # Element-wise sum / é€å…ƒç´ æ±‚å’Œ
    "avg": lambda tensors: torch.stack(tensors).mean(dim=0),  # Element-wise average / é€å…ƒç´ å‡å€¼
    "mul": lambda tensors: torch.stack(tensors).prod(dim=0),  # Element-wise multiplication / é€å…ƒç´ ç›¸ä¹˜
    "max": lambda tensors: torch.stack(tensors).max(dim=0)[0],  # Element-wise maximum / é€å…ƒç´ æœ€å¤§å€¼
    "min": lambda tensors: torch.stack(tensors).min(dim=0)[0],  # Element-wise minimum / é€å…ƒç´ æœ€å°å€¼
}

# Edge Input Functions / è¶…è¾¹è¾“å…¥å‡½æ•°ï¼ˆå¤šèŠ‚ç‚¹â†’è¶…è¾¹å•è¾“å…¥ï¼‰
MHD_EDGE_IN_FUNCS: FuncMapping = {
    "concat": lambda tensors, indices: torch.cat(
        [t for _, t in sorted(zip(indices, tensors), key=lambda x: x[0])],
        dim=1
    ),  # Concatenate by topology order / æŒ‰æ‹“æ‰‘é¡ºåºæ‹¼æ¥
    "matmul": lambda tensors, indices: torch.matmul(
        *[t for _, t in sorted(zip(indices, tensors), key=lambda x: x[0])]
    ),  # Matrix multiplication by topology order / æŒ‰æ‹“æ‰‘é¡ºåºçŸ©é˜µä¹˜æ³•
}

# Edge Output Functions / è¶…è¾¹è¾“å‡ºå‡½æ•°ï¼ˆè¶…è¾¹å•è¾“å‡ºâ†’å¤šèŠ‚ç‚¹ï¼‰
MHD_EDGE_OUT_FUNCS: FuncMapping = {
    "split": lambda x, indices, channel_sizes: torch.split(x, channel_sizes, dim=1),
    "svd": lambda x, indices, channel_sizes: list(torch.svd(x))[:len(indices)],  # SVD decomposition / SVDåˆ†è§£
    "lr": lambda x, indices, channel_sizes: [x @ x.t(), x.t() @ x][:len(indices)],  # LR decomposition / LRåˆ†è§£
}

# ===================== Hypergraph Core Structure / è¶…å›¾æ ¸å¿ƒæ•°æ®ç»“æ„ =====================
@dataclass(unsafe_hash=True)  # æ”¯æŒå“ˆå¸Œï¼Œå¯æ”¾å…¥seté›†åˆ
class MHD_Node:
    """
    Hypergraph Node / è¶…å›¾èŠ‚ç‚¹
    Attributes / å±æ€§:
        id: Unique identifier (column index of incidence matrix) / å”¯ä¸€æ ‡è¯†ï¼ˆå…³è”çŸ©é˜µåˆ—ç´¢å¼•ï¼‰
        name: Callable name for easy access / ä¾¿äºè°ƒç”¨çš„åç§°
        value: Node feature tensor (initialized value, updated in forward) / èŠ‚ç‚¹ç‰¹å¾å¼ é‡ï¼ˆåˆå§‹åŒ–å€¼ï¼Œå‰å‘ä¼ æ’­æ›´æ–°ï¼‰
        func: Function mapping {"head": func_name, "tail": func_name} / å‡½æ•°æ˜ å°„ï¼ˆå¤´/å°¾èŠ‚ç‚¹å‡½æ•°ï¼‰
    """
    id: int
    name: str
    value: torch.Tensor
    func: Dict[str, str] = field(default_factory=lambda: {"head": "share", "tail": "sum"}, hash=False)

@dataclass(unsafe_hash=True)  # æ”¯æŒå“ˆå¸Œï¼Œå¯æ”¾å…¥seté›†åˆ
class MHD_Edge:
    """
    Hypergraph Edge / è¶…å›¾è¾¹
    Attributes / å±æ€§:
        id: Unique identifier (row index of incidence matrix) / å”¯ä¸€æ ‡è¯†ï¼ˆå…³è”çŸ©é˜µè¡Œç´¢å¼•ï¼‰
        name: Callable name for easy access / ä¾¿äºè°ƒç”¨çš„åç§°
        value: Operation sequence (mix of string method / torch Module) / æ“ä½œåºåˆ—ï¼ˆå­—ç¬¦ä¸²æ–¹æ³•/å¼ é‡æ¨¡å—æ··åˆï¼‰
        func: Dict[str, str] = field(default_factory=lambda: {"in": "concat", "out": "split"}, hash=False)
    """
    id: int
    name: str
    value: List[Union[str, nn.Module]] = field(hash=False)
    func: Dict[str, str] = field(default_factory=lambda: {"in": "concat", "out": "split"}, hash=False)

@dataclass
class MHD_Topo:
    """
    Hypergraph Topology / è¶…å›¾æ‹“æ‰‘
    Attributes / å±æ€§:
        id: Topology ID / æ‹“æ‰‘ID
        name: Topology name / æ‹“æ‰‘åç§°
        value: Incidence matrix (int tensor, shape: [num_edges, num_nodes]) / å…³è”çŸ©é˜µï¼ˆæ•´å‹å¼ é‡ï¼‰
    """
    id: int
    name: str
    value: torch.Tensor

# ===================== Core Utility Functions / æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def mhd_parse_string_method(method_str: str, x: Tensor) -> Tensor:
    """
    Parse and execute tensor method from string / è§£æå­—ç¬¦ä¸²å½¢å¼çš„å¼ é‡æ–¹æ³•å¹¶æ‰§è¡Œ
    Example / ç¤ºä¾‹:
        "__add__(3)" â†’ x.__add__(3)
        "reshape(1, 4, -1)" â†’ x.reshape(1, 4, -1)
        "squeeze(0)" â†’ x.squeeze(0)
    """
    if not isinstance(method_str, str):
        return x
    if '(' in method_str and ')' in method_str:
        method_name, args_str = method_str.split('(', 1)
        args_str = args_str.rstrip(')')
        args = []
        kwargs = {}
        if args_str:
            for arg in args_str.split(','):
                arg = arg.strip()
                if not arg:
                    continue
                if '=' in arg:
                    k, v = arg.split('=', 1)
                    kwargs[k.strip()] = eval(v.strip())
                else:
                    args.append(eval(arg.strip()))
        if hasattr(x, method_name):
            return getattr(x, method_name)(*args, **kwargs)
        else:
            raise ValueError(f"Tensor has no method: {method_name} / å¼ é‡æ— æ­¤æ–¹æ³•: {method_name}")
    else:
        if hasattr(x, method_str):
            return getattr(x, method_str)()
        else:
            raise ValueError(f"Tensor has no method: {method_str} / å¼ é‡æ— æ­¤æ–¹æ³•: {method_str}")

def mhd_register_head_func(name: str, func: Callable[[Tensor], Tensor]) -> None:
    """Register new head node function / æ³¨å†Œæ–°çš„å¤´èŠ‚ç‚¹å‡½æ•°"""
    MHD_NODE_HEAD_FUNCS[name] = func

def mhd_register_tail_func(name: str, func: Callable[[List[Tensor]], Tensor]) -> None:
    """Register new tail node function / æ³¨å†Œæ–°çš„å°¾èŠ‚ç‚¹å‡½æ•°"""
    MHD_NODE_TAIL_FUNCS[name] = func

def mhd_register_edge_in_func(name: str, func: Callable[[List[Tensor], List[int]], Tensor]) -> None:
    """Register new edge input function / æ³¨å†Œæ–°çš„è¶…è¾¹è¾“å…¥å‡½æ•°"""
    MHD_EDGE_IN_FUNCS[name] = func

def mhd_register_edge_out_func(name: str, func: Callable[[Tensor, List[int], List[int]], List[Tensor]]) -> None:
    """Register new edge output function / æ³¨å†Œæ–°çš„è¶…è¾¹è¾“å‡ºå‡½æ•°"""
    MHD_EDGE_OUT_FUNCS[name] = func

# ===================== Core Network Modules / æ ¸å¿ƒç½‘ç»œæ¨¡å— =====================
class DNet(nn.Module):
    """
    Dynamic Network / åŠ¨æ€ç½‘ç»œ
    Function / åŠŸèƒ½:
        Execute hypergraph edge operation sequence / æ‰§è¡Œè¶…å›¾è¾¹çš„æ“ä½œåºåˆ—
    Support / æ”¯æŒ:
        PyTorch Module + Any Tensor Method String / PyTorchæ¨¡å— + ä»»æ„å¼ é‡æ–¹æ³•å­—ç¬¦ä¸²
    """
    def __init__(self, operations: List[Union[str, nn.Module]]):
        super().__init__()
        self.operations = nn.ModuleList()
        self.str_ops = []
        for op in operations:
            if isinstance(op, nn.Module):
                self.operations.append(op)
                self.str_ops.append(None)
            elif isinstance(op, str):
                self.operations.append(nn.Identity())
                self.str_ops.append(op)
            else:
                raise ValueError(f"Unsupported operation type: {type(op)} / ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {type(op)}")

    def forward(self, x: Tensor) -> Tensor:
        """Forward propagation / å‰å‘ä¼ æ’­"""
        for op, str_op in zip(self.operations, self.str_ops):
            if str_op is not None:
                x = mhd_parse_string_method(str_op, x)
            else:
                x = op(x)
        return x

class HDNet(nn.Module):
    """
    Hypergraph Dynamic Network / è¶…å›¾åŠ¨æ€ç½‘ç»œ
    Core Logic / æ ¸å¿ƒé€»è¾‘:
        Head Node â†’ Aggregate by order â†’ Edge Operation â†’ Distribute by order â†’ Tail Node
        å¤´èŠ‚ç‚¹ â†’ æŒ‰é¡ºåºèšåˆ â†’ è¶…è¾¹æ“ä½œ â†’ æŒ‰é¡ºåºåˆ†å‘ â†’ å°¾èŠ‚ç‚¹
    """
    def __init__(self, nodes: set[MHD_Node], edges: set[MHD_Edge], topo: MHD_Topo):
        super().__init__()
        # Index mapping / ç´¢å¼•æ˜ å°„
        self.node_id2obj = {node.id: node for node in nodes}
        self.node_name2id = {node.name: node.id for node in nodes}
        self.edge_id2obj = {edge.id: edge for edge in edges}
        self.edge_name2id = {edge.name: edge.id for edge in edges}

        # Topology validation / æ‹“æ‰‘éªŒè¯
        self.topo = topo
        self._validate_topo()

        # Initialize edge operation networks / åˆå§‹åŒ–è¶…è¾¹æ“ä½œç½‘ç»œ
        self.edge_nets = nn.ModuleDict()
        for edge in edges:
            self.edge_nets[edge.name] = DNet(edge.value)

        # Node feature cache / èŠ‚ç‚¹ç‰¹å¾ç¼“å­˜
        self.node_values = {node.id: node.value.clone() for node in nodes}

    def _validate_topo(self) -> None:
        """Validate topology matrix / éªŒè¯æ‹“æ‰‘çŸ©é˜µåˆæ³•æ€§"""
        num_edges = len(self.edge_id2obj)
        num_nodes = len(self.node_id2obj)
        # Dimension validation / ç»´åº¦éªŒè¯
        if self.topo.value.shape != (num_edges, num_nodes):
            raise ValueError(
                f"Topology matrix dimension mismatch / æ‹“æ‰‘çŸ©é˜µç»´åº¦ä¸åŒ¹é…: "
                f"Expected ({num_edges}, {num_nodes}), Got {self.topo.value.shape}"
            )
        # Type validation / ç±»å‹éªŒè¯
        dtype_name = str(self.topo.value.dtype).lower()
        if not any(keyword in dtype_name for keyword in ['int', 'long', 'short']):
            raise ValueError(f"Topology matrix must be integer type / æ‹“æ‰‘çŸ©é˜µå¿…é¡»ä¸ºæ•´å‹: Got {self.topo.value.dtype}")

    def _get_channel_sizes(self, x: Tensor, indices: List[int]) -> List[int]:
        """
        Calculate channel sizes by topology order / æŒ‰æ‹“æ‰‘é¡ºåºè®¡ç®—é€šé“åˆ†é…å°ºå¯¸
        """
        total_channels = x.shape[1]
        num_nodes = len(indices)
        if num_nodes == 0:
            return []
        # Sort by topology order (absolute value) / æŒ‰æ‹“æ‰‘é¡ºåºæ’åºï¼ˆç»å¯¹å€¼ï¼‰
        sorted_indices = sorted(indices, key=lambda x: abs(x))
        base_channels = total_channels // num_nodes
        channel_sizes = [base_channels] * num_nodes
        # Distribute remaining channels (follow topology order) / åˆ†é…å‰©ä½™é€šé“ï¼ˆæŒ‰æ‹“æ‰‘é¡ºåºï¼‰
        remaining = total_channels % num_nodes
        for i in range(remaining):
            channel_sizes[i] += 1
        # Map back to original order / æ˜ å°„å›åŸå§‹é¡ºåº
        size_map = {idx: size for idx, size in zip(sorted_indices, channel_sizes)}
        return [size_map[idx] for idx in indices]

    def forward(self, input_node_names: List[str], input_tensors: List[Tensor]) -> Dict[str, Tensor]:
        """
        Forward propagation / å‰å‘ä¼ æ’­
        Args:
            input_node_names: Input node name list / è¾“å…¥èŠ‚ç‚¹åç§°åˆ—è¡¨
            input_tensors: Input feature tensor list / è¾“å…¥ç‰¹å¾å¼ é‡åˆ—è¡¨
        Returns:
            Node name â†’ feature tensor mapping / èŠ‚ç‚¹åç§°åˆ°ç‰¹å¾å¼ é‡çš„æ˜ å°„
        """
        # Initialize input node features / åˆå§‹åŒ–è¾“å…¥èŠ‚ç‚¹ç‰¹å¾
        for name, tensor in zip(input_node_names, input_tensors):
            node_id = self.node_name2id[name]
            self.node_values[node_id] = tensor.to(dtype=torch.float32)

        # Iterate all edges (by ID order) / éå†æ‰€æœ‰è¶…è¾¹ï¼ˆæŒ‰IDé¡ºåºï¼‰
        for edge_id in sorted(self.edge_id2obj.keys()):
            edge = self.edge_id2obj[edge_id]
            edge_net = self.edge_nets[edge.name]
            edge_topo = self.topo.value[edge_id]

            # ========== ä¿®å¤ï¼šç”¨PyTorch APIæ›¿ä»£Pythonéå†ï¼Œæ¶ˆé™¤TraceWarning ==========
            # å¤´èŠ‚ç‚¹è§£æï¼ˆæ— Pythonéå†/ç±»å‹è½¬æ¢ï¼‰
            head_mask = edge_topo < 0
            head_node_ids = torch.where(head_mask)[0].tolist()
            head_node_orders = edge_topo[head_mask].tolist()  # å¤´èŠ‚ç‚¹é¡ºåºï¼ˆè¾“å…¥é¡ºåºï¼‰
            
            # å°¾èŠ‚ç‚¹è§£æï¼ˆæ— Pythonéå†/ç±»å‹è½¬æ¢ï¼‰
            tail_mask = edge_topo > 0
            tail_node_ids = torch.where(tail_mask)[0].tolist()
            tail_node_orders = edge_topo[tail_mask].tolist()  # å°¾èŠ‚ç‚¹é¡ºåºï¼ˆè¾“å‡ºé¡ºåºï¼‰

            # Get head node features / è·å–å¤´èŠ‚ç‚¹ç‰¹å¾
            head_tensors = []
            for node_id in head_node_ids:
                node = self.node_id2obj[node_id]
                head_func_name = node.func.get("head", "share")
                if head_func_name not in MHD_NODE_HEAD_FUNCS:
                    raise ValueError(
                        f"Unknown head function / æœªçŸ¥å¤´èŠ‚ç‚¹å‡½æ•°: {head_func_name}, "
                        f"Registered / å·²æ³¨å†Œ: {list(MHD_NODE_HEAD_FUNCS.keys())}"
                    )
                head_tensor = MHD_NODE_HEAD_FUNCS[head_func_name](self.node_values[node_id])
                head_tensors.append(head_tensor)

            # Edge input aggregation / è¶…è¾¹è¾“å…¥èšåˆ
            edge_in_func_name = edge.func.get("in", "concat")
            if edge_in_func_name not in MHD_EDGE_IN_FUNCS:
                raise ValueError(
                    f"Unknown edge input function / æœªçŸ¥è¶…è¾¹è¾“å…¥å‡½æ•°: {edge_in_func_name}, "
                    f"Registered / å·²æ³¨å†Œ: {list(MHD_EDGE_IN_FUNCS.keys())}"
                )
            # FIX 1: è¾“å…¥èšåˆç”¨å¤´èŠ‚ç‚¹é¡ºåºï¼ˆhead_node_ordersï¼‰ï¼Œä¸æ˜¯å°¾èŠ‚ç‚¹
            edge_input = MHD_EDGE_IN_FUNCS[edge_in_func_name](head_tensors, head_node_orders)

            # Execute edge operations / æ‰§è¡Œè¶…è¾¹æ“ä½œ
            edge_output = edge_net(edge_input)

            # Edge output distribution / è¶…è¾¹è¾“å‡ºåˆ†å‘
            edge_out_func_name = edge.func.get("out", "split")
            if edge_out_func_name not in MHD_EDGE_OUT_FUNCS:
                raise ValueError(
                    f"Unknown edge output function / æœªçŸ¥è¶…è¾¹è¾“å‡ºå‡½æ•°: {edge_out_func_name}, "
                    f"Registered / å·²æ³¨å†Œ: {list(MHD_EDGE_OUT_FUNCS.keys())}"
                )
            # Split by topology order / æŒ‰æ‹“æ‰‘é¡ºåºæ‹†åˆ†
            if edge_out_func_name == "split":
                # FIX 2: è¾“å‡ºæ‹†åˆ†ç”¨å°¾èŠ‚ç‚¹é¡ºåºï¼ˆtail_node_ordersï¼‰ï¼Œä¸æ˜¯å¤´èŠ‚ç‚¹
                channel_sizes = self._get_channel_sizes(edge_output, tail_node_orders)
                tail_tensors = MHD_EDGE_OUT_FUNCS[edge_out_func_name](edge_output, tail_node_orders, channel_sizes)
            else:
                tail_tensors = MHD_EDGE_OUT_FUNCS[edge_out_func_name](edge_output, tail_node_orders, [])

            # Update tail node features / æ›´æ–°å°¾èŠ‚ç‚¹ç‰¹å¾
            for node_id, tensor in zip(tail_node_ids, tail_tensors):
                node = self.node_id2obj[node_id]
                tail_func_name = node.func.get("tail", "sum")
                if tail_func_name not in MHD_NODE_TAIL_FUNCS:
                    raise ValueError(
                        f"Unknown tail function / æœªçŸ¥å°¾èŠ‚ç‚¹å‡½æ•°: {tail_func_name}, "
                        f"Registered / å·²æ³¨å†Œ: {list(MHD_NODE_TAIL_FUNCS.keys())}"
                    )
                if node_id in self.node_values:
                    self.node_values[node_id] = MHD_NODE_TAIL_FUNCS[tail_func_name](
                        [self.node_values[node_id], tensor]
                    )
                else:
                    self.node_values[node_id] = tensor

        # Return name-based mapping / è¿”å›åç§°æ˜ å°„ç»“æœ
        return {
            self.node_id2obj[node_id].name: tensor
            for node_id, tensor in self.node_values.items()
        }

class MHDNet(nn.Module):
    """
    Multi Hypergraph Dynamic Network / å¤šè¶…å›¾åŠ¨æ€ç½‘ç»œ
    Function / åŠŸèƒ½:
        Integrate multiple HDNet into global hypergraph / æ•´åˆå¤šä¸ªHDNetä¸ºå…¨å±€è¶…å›¾
    """
    def __init__(
        self,
        sub_hdnets: Dict[str, HDNet],
        node_mapping: List[Tuple[str, str, str]],  # (global_name, sub_name, sub_node_name)
        in_nodes: List[str],
        out_nodes: List[str],
        onnx_save_path: Optional[str] = None,
    ):
        super().__init__()
        self.sub_hdnets = nn.ModuleDict(sub_hdnets)
        self.in_nodes = in_nodes
        self.out_nodes = out_nodes

        # Build global hypergraph / æ„å»ºå…¨å±€è¶…å›¾
        self.global_hdnet = self._build_global_hypergraph(node_mapping)

        # Export ONNX / æ¢å¤ONNXå¯¼å‡ºåŠŸèƒ½
        if onnx_save_path:
            self._export_to_onnx(onnx_save_path)

    def _build_global_hypergraph(self, node_mapping: List[Tuple[str, str, str]]) -> HDNet:
        """Build global hypergraph / æ„å»ºå…¨å±€è¶…å›¾ï¼ˆä»…å¤„ç†ä½ æ˜ å°„çš„èŠ‚ç‚¹ï¼Œä¸è¡¥å…¨ã€ä¸ç”Ÿæˆé»˜è®¤ï¼‰"""
        # Initialize counters / åˆå§‹åŒ–è®¡æ•°å™¨
        node_id_counter = 0
        edge_id_counter = 0

        # Mapping tables / æ˜ å°„è¡¨
        sub2global_node = {}  # (sub_name, sub_node_name) â†’ (global_id, global_name)
        sub2global_edge = {}  # (sub_name, sub_edge_name) â†’ global_edge_id

        # Collect global nodes / ä»…æ”¶é›†ä½ æ˜¾å¼æ˜ å°„çš„èŠ‚ç‚¹ï¼ˆä¸è¡¥å…¨ï¼‰
        global_nodes = set()
        for global_node_name, sub_name, sub_node_name in node_mapping:
            key = (sub_name, sub_node_name)
            if key not in sub2global_node:
                sub_hdnet = self.sub_hdnets[sub_name]
                sub_node_id = sub_hdnet.node_name2id[sub_node_name]
                sub_node = sub_hdnet.node_id2obj[sub_node_id]
                global_node = MHD_Node(
                    id=node_id_counter,
                    name=global_node_name,
                    value=sub_node.value.clone(),
                    func=sub_node.func.copy()
                )
                global_nodes.add(global_node)
                sub2global_node[key] = (node_id_counter, global_node_name)
                node_id_counter += 1

        # Collect global edges and topology / æ”¶é›†å…¨å±€è¾¹å’Œæ‹“æ‰‘ï¼ˆä»…å¤„ç†æ˜ å°„èŠ‚ç‚¹ï¼‰
        global_edges = set()
        global_topo_data = []
        
        for sub_name, sub_hdnet in self.sub_hdnets.items():
            for sub_edge_id in sorted(sub_hdnet.edge_id2obj.keys()):
                sub_edge = sub_hdnet.edge_id2obj[sub_edge_id]
                sub_edge_name = sub_edge.name
                key = (sub_name, sub_edge_name)
                if key not in sub2global_edge:
                    sub2global_edge[key] = edge_id_counter
                    # Create global edge / åˆ›å»ºå…¨å±€è¾¹
                    global_edge = MHD_Edge(
                        id=edge_id_counter,
                        name=f"{sub_name}_{sub_edge_name}",
                        value=sub_edge.value.copy(),
                        func=sub_edge.func.copy()
                    )
                    global_edges.add(global_edge)

                    # Convert sub topology to global topology / è½¬æ¢å­æ‹“æ‰‘åˆ°å…¨å±€æ‹“æ‰‘ï¼ˆä»…æ˜ å°„èŠ‚ç‚¹ï¼‰
                    sub_topo_row = sub_hdnet.topo.value[sub_edge_id]
                    # FIX 3: å…¨å±€æ‹“æ‰‘è¡Œç»´åº¦ = ä»…ä½ æ˜ å°„çš„èŠ‚ç‚¹æ•°ï¼ˆnode_id_counterï¼‰
                    global_topo_row = torch.zeros(node_id_counter, dtype=torch.int64)
                    
                    # Map sub node ID to global node ID / ä»…æ˜ å°„ä½ æŒ‡å®šçš„èŠ‚ç‚¹ï¼Œæœªæ˜ å°„çš„ç›´æ¥è·³è¿‡ï¼ˆæŠ¥é”™ç”±ä½ æ§åˆ¶ï¼‰
                    for sub_node_id, val in enumerate(sub_topo_row):
                        if val == 0:
                            continue
                        sub_node_name = sub_hdnet.node_id2obj[sub_node_id].name
                        map_key = (sub_name, sub_node_name)
                        if map_key in sub2global_node:
                            global_node_id, _ = sub2global_node[map_key]
                            global_topo_row[global_node_id] = val

                    global_topo_data.append(global_topo_row)
                    edge_id_counter += 1

        # Create global topology / åˆ›å»ºå…¨å±€æ‹“æ‰‘
        if global_topo_data:
            global_topo_value = torch.stack(global_topo_data)
        else:
            global_topo_value = torch.empty(0, 0, dtype=torch.int64)
        
        global_topo = MHD_Topo(
            id=0,
            name="global_topo",
            value=global_topo_value
        )

        # Create global HDNet / åˆ›å»ºå…¨å±€HDNet
        return HDNet(nodes=global_nodes, edges=global_edges, topo=global_topo)

    def _export_to_onnx(self, onnx_save_path: str) -> None:
        """Export model to ONNX format / å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼"""
        self.eval()
        # Build input shapes / æ„å»ºè¾“å…¥å½¢çŠ¶
        input_shapes = []
        for node_name in self.in_nodes:
            for node in self.global_hdnet.node_id2obj.values():
                if node.name == node_name:
                    input_shapes.append(node.value.shape)
                    break
        # Generate example inputs / ç”Ÿæˆç¤ºä¾‹è¾“å…¥
        inputs = [torch.randn(*shape) for shape in input_shapes]
        dynamic_axes = {
            **{f"input_{node}": {0: "batch_size"} for node in self.in_nodes},
            **{f"output_{node}": {0: "batch_size"} for node in self.out_nodes}
        }
        # Export / å¯¼å‡ºï¼ˆç§»é™¤æ— æ•ˆçš„warningså‚æ•°ï¼‰
        abs_onnx_path = os.path.abspath(onnx_save_path)
        # è‡ªåŠ¨åˆ¤æ–­PyTorchç‰ˆæœ¬ï¼Œé€‰æ‹©åˆé€‚çš„opset
        torch_version = [int(v) for v in torch.__version__.split('.')[:2]]
        if torch_version >= [2, 0]:
            opset_version = 17
        else:
            opset_version = 11
        torch.onnx.export(
            self,
            inputs,
            abs_onnx_path,
            input_names=[f"input_{node}" for node in self.in_nodes],
            output_names=[f"output_{node}" for node in self.out_nodes],
            dynamic_axes=dynamic_axes,
            keep_initializers_as_inputs=True,
            do_constant_folding=False,
            opset_version=opset_version
        )
        print(f"Model exported to / æ¨¡å‹å·²å¯¼å‡ºè‡³: {abs_onnx_path} (opset={opset_version})")

    def forward(self, inputs: List[Tensor]) -> List[Tensor]:
        """Global forward propagation / å…¨å±€å‰å‘ä¼ æ’­"""
        if len(inputs) != len(self.in_nodes):
            raise ValueError(
                f"Input count mismatch / è¾“å…¥æ•°é‡ä¸åŒ¹é…: "
                f"Expected {len(self.in_nodes)}, Got {len(inputs)}"
            )
        # Execute global hypergraph forward / æ‰§è¡Œå…¨å±€è¶…å›¾å‰å‘ä¼ æ’­
        global_outputs = self.global_hdnet.forward(self.in_nodes, inputs)
        # Extract output nodes / æå–è¾“å‡ºèŠ‚ç‚¹
        outputs = [global_outputs[node] for node in self.out_nodes]
        return outputs

# ===================== Example / ç¤ºä¾‹ï¼ˆä»…ä¿®æ”¹æ­¤å¤„ï¼Œæ ¸å¿ƒé€»è¾‘ä¸å˜ï¼‰ =====================
def example_mhdnet():
    """
    Example: Single HDNet with multiple dimensions (å•ä¸ªHDNetå†…åŒ…å«å¤šç§ç»´åº¦)
    ä¿®å¤ç‚¹ï¼šåœ¨exampleä¸­é€šè¿‡unsqueeze/reshapeåšç»´åº¦è½¬æ¢ï¼Œç¡®ä¿concatæ—¶ç»´åº¦ä¸€è‡´
    """
    # ===================== å•ä¸ªHDNetåŒ…å«å¤šç§ç»´åº¦èŠ‚ç‚¹ =====================
    # HDNet1: å†…éƒ¨åŒ…å«3D+2DèŠ‚ç‚¹ï¼ˆé€šè¿‡unsqueezeå°†2Dè½¬ä¸º3Dï¼Œç¡®ä¿concatç»´åº¦ä¸€è‡´ï¼‰
    hdnet1_nodes = {
        # 3DèŠ‚ç‚¹ (batch, channel, depth, height, width)
        MHD_Node(
            id=0,
            name="node3d_1",
            value=torch.randn(1, 4, 8, 16, 16),
            func={"head": "share", "tail": "avg"}
        ),
        # 2DèŠ‚ç‚¹ â†’ é€šè¿‡unsqueeze(2)æ·»åŠ depthç»´åº¦ï¼Œè½¬ä¸º3D (1,4,1,16,16)
        MHD_Node(
            id=1,
            name="node2d_1",
            value=torch.randn(1, 4, 16, 16).unsqueeze(2).repeat(1,1,8,1,1),  # 2Dâ†’3D: (1,4,16,16) â†’ (1,4,8,16,16)
            func={"head": "share", "tail": "avg"}
        ),
        # è¾“å‡ºèŠ‚ç‚¹ï¼ˆ3Dï¼‰
        MHD_Node(
            id=2,
            name="output3d_1",
            value=torch.randn(1, 4, 8, 16, 16),
            func={"head": "share", "tail": "avg"}
        )
    }

    # HDNet1çš„è¾¹ï¼šä¿®å¤å·ç§¯é€šé“æ•°ï¼ˆè¾“å…¥8é€šé“ â†’ è¾“å‡º4é€šé“ï¼‰
    hdnet1_edges = {
        MHD_Edge(
            id=0,
            name="edge1",
            value=[
                nn.Conv3d(8, 4, kernel_size=3, padding=1, bias=False),  # 4+4=8è¾“å…¥é€šé“ â†’ 4è¾“å‡ºé€šé“
                "relu_()"
            ],
            func={"in": "concat", "out": "split"}
        )
    }

    # HDNet1æ‹“æ‰‘çŸ©é˜µ
    hdnet1_topo = MHD_Topo(
        id=0,
        name="topo1",
        value=torch.tensor([[-1, -2, 1]], dtype=torch.int64)  # node3d_1(-1), node2d_1(-2) â†’ output3d_1(1)
    )

    # åˆ›å»ºHDNet1
    hdnet1 = HDNet(nodes=hdnet1_nodes, edges=hdnet1_edges, topo=hdnet1_topo)

    # ===================== HDNet2: å†…éƒ¨åŒ…å«2D+1DèŠ‚ç‚¹ï¼ˆé€šè¿‡reshapeå°†1Dè½¬ä¸º2Dï¼‰ =====================
    hdnet2_nodes = {
        # 2DèŠ‚ç‚¹ (batch, channel, height, width)
        MHD_Node(
            id=0,
            name="node2d_2",
            value=torch.randn(1, 8, 16, 16),
            func={"head": "share", "tail": "sum"}
        ),
        # 1DèŠ‚ç‚¹ â†’ é€šè¿‡reshapeè½¬ä¸º2D (1,8,16,16)
        MHD_Node(
            id=1,
            name="node1d_2",
            value=torch.randn(1, 8, 256).reshape(1,8,16,16),  # 1Dâ†’2D: (1,8,256) â†’ (1,8,16,16)
            func={"head": "share", "tail": "sum"}
        ),
        # è¾“å‡ºèŠ‚ç‚¹ï¼ˆ2Dï¼‰
        MHD_Node(
            id=2,
            name="output2d_2",
            value=torch.randn(1, 8, 16, 16),
            func={"head": "share", "tail": "sum"}
        )
    }

    # HDNet2çš„è¾¹ï¼šä¿®å¤æ“ä½œåºåˆ—ï¼ˆç»Ÿä¸€2Dæ“ä½œï¼‰
    hdnet2_edges = {
        MHD_Edge(
            id=0,
            name="edge2",
            value=[
                nn.Conv2d(16, 8, kernel_size=3, padding=1, bias=False),  # 8+8=16è¾“å…¥é€šé“ â†’ 8è¾“å‡ºé€šé“
                "relu_()"
            ],
            func={"in": "concat", "out": "split"}
        )
    }

    # HDNet2æ‹“æ‰‘çŸ©é˜µ
    hdnet2_topo = MHD_Topo(
        id=0,
        name="topo2",
        value=torch.tensor([[-1, -2, 1]], dtype=torch.int64)  # node2d_2(-1), node1d_2(-2) â†’ output2d_2(1)
    )

    # åˆ›å»ºHDNet2
    hdnet2 = HDNet(nodes=hdnet2_nodes, edges=hdnet2_edges, topo=hdnet2_topo)

    # ===================== å¤šHDNetæ•´åˆ =====================
    sub_hdnets = {
        "hdnet1": hdnet1,
        "hdnet2": hdnet2
    }

    # èŠ‚ç‚¹æ˜ å°„
    node_mapping = [
        ("global_3d", "hdnet1", "node3d_1"),
        ("global_2d", "hdnet1", "node2d_1"),
        ("global_2d_2", "hdnet2", "node2d_2"),
        ("global_1d", "hdnet2", "node1d_2"),
        ("final_3d", "hdnet1", "output3d_1"),
        ("final_2d", "hdnet2", "output2d_2")
    ]

    # åˆ›å»ºMHDNetï¼ˆæ¢å¤ONNXå¯¼å‡ºï¼‰
    model = MHDNet(
        sub_hdnets=sub_hdnets,
        node_mapping=node_mapping,
        in_nodes=["global_3d", "global_2d", "global_2d_2", "global_1d"],
        out_nodes=["final_3d", "final_2d"],
        onnx_save_path="MHDNodeF_example.onnx"
    )

    # è¿è¡Œç¤ºä¾‹ï¼ˆä¸¥æ ¼è¿˜åŸåŸå§‹è¾“å‡ºæ ¼å¼ï¼‰
    model.eval()
    with torch.no_grad():
        # è¾“å…¥ï¼ˆä¸¥æ ¼æ§åˆ¶å°ºå¯¸åŒ¹é…ï¼Œåšç»´åº¦è½¬æ¢ï¼‰
        input_3d = torch.randn(1, 4, 8, 16, 16)
        input_2d_1 = torch.randn(1, 4, 16, 16).unsqueeze(2).repeat(1,1,8,1,1)  # 2Dâ†’3D
        input_2d_2 = torch.randn(1, 8, 16, 16)
        input_1d = torch.randn(1, 8, 256).reshape(1,8,16,16)  # 1Dâ†’2D

        # å‰å‘ä¼ æ’­
        outputs = model([input_3d, input_2d_1, input_2d_2, input_1d])

        # ä¸¥æ ¼æŒ‰åŸå§‹ä»£ç è¾“å‡ºæ ¼å¼æ‰“å°
        print("HDNet1 Output Shape:", hdnet1.forward(["node3d_1", "node2d_1"], [input_3d, input_2d_1])["output3d_1"].shape)
        print("HDNet2 Output Shape:", hdnet2.forward(["node2d_2", "node1d_2"], [input_2d_2, input_1d])["output2d_2"].shape)
        print("MHDNet Final Output Shapes:", [o.shape for o in outputs])

if __name__ == "__main__":
    # æ‰“å°PyTorchç‰ˆæœ¬
    print(f"PyTorch Version: {torch.__version__}")
    # è¿è¡Œç¤ºä¾‹
    example_mhdnet()
