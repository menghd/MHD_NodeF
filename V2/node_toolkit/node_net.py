# -*- coding: utf-8 -*-
"""
Hypergraph Neural Network (HDNet/MHDNet)
è¶…å›¾ç¥ç»ç½‘ç»œæ ¸å¿ƒæ¡†æ¶ (Hypergraph Neural Network Core Framework)

æ ¸å¿ƒè®¾è®¡ç›®æ ‡ (Core Design Goals):
1. æ„å»ºå¤šå­å›¾è¶…å›¾ç½‘ç»œï¼ˆå­å›¾åˆå¹¶ä¸ºå…¨å±€è¶…å›¾ï¼‰
   Build multi-subgraph hypergraph network (subgraphs merged into global hypergraph)
2. æ‹“æ‰‘é©±åŠ¨çš„å¼ é‡æµï¼ˆæ— å†—ä½™å¼ é‡è½¬æ¢ï¼‰
   Topology-driven tensor flow (no redundant tensor conversion)
3. å“ˆå¸Œå†²çªè§£å†³ï¼ˆå®‰å…¨çš„å¼ é‡å­—æ®µå¤„ç†ï¼‰
   Hash conflict resolution (safe tensor field processing)
4. æ˜“å¯¼å‡ºæ¶æ„ï¼ˆå¯åºåˆ—åŒ–æ•°æ®ç»“æ„ï¼‰
   Exportable architecture (serializable data structures)
5. ç»“æ„åŒ–æ‹“æ‰‘çŸ©é˜µï¼ˆæ”¯æŒæ‰©å±•å±æ€§ï¼‰
   Structured topology matrix (support extended attributes)

æ ¸å¿ƒç‰¹æ€§ (Core Features):
- è¶…å›¾èŠ‚ç‚¹/è¾¹æŠ½è±¡ï¼ˆç›´æ¥å¼ é‡èµ‹å€¼ï¼‰
  Hypergraph node/edge abstraction (direct tensor assignment)
- åŸºäºæ‹“æ‰‘æ’åºçš„æœ‰å‘æ— ç¯å›¾å‰å‘ä¼ æ’­
  Topological sort-based DAG forward propagation
- åŠ¨æ€è¾¹æ“ä½œç½‘ç»œï¼ˆæ”¯æŒå­—ç¬¦ä¸²/Moduleç±»å‹æ“ä½œï¼‰
  Dynamic edge operation network (support string/Module type operations)
- å¤šå­å›¾åˆ°å…¨å±€å›¾çš„æ˜ å°„ï¼ˆè‡ªåŠ¨æ‹“æ‰‘åˆå¹¶ï¼‰
  Multi-subgraph to global graph mapping (automatic topology merging)

Author: Your Name
Date: 2026
Version: 2.0
License: MIT
"""

import torch
import torch.nn as nn
from typing import List, Dict, Tuple, Optional, Union, Any, Callable, TypeVar, Set
from dataclasses import dataclass, field
from collections import defaultdict, deque
import warnings
import re
from collections import namedtuple

# å…¨å±€é…ç½® (Global Configuration)
warnings.filterwarnings("ignore", category=torch.jit.TracerWarning)
warnings.filterwarnings("ignore", category=UserWarning)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DTYPE = torch.float32

# ç±»å‹å®šä¹‰ (Type Definitions)
Tensor = TypeVar('Tensor', bound=torch.Tensor)

# ç»“æ„åŒ–æ‹“æ‰‘å±æ€§ (Structured Topology Attributes)
TopoAttr = namedtuple("TopoAttr", ["role", "sort", "ext"], defaults=[None])

# å…¨å±€å‡½æ•°æ³¨å†Œè¡¨ (Global Function Registry)
# èŠ‚ç‚¹å¤´å‡½æ•°ï¼ˆè¾¹è¾“å…¥å‰çš„å¼ é‡é¢„å¤„ç†ï¼‰
# Node head functions (tensor preprocessing before edge input)
MHD_NODE_HEAD_FUNCS: Dict[str, Callable[..., Any]] = {
    "share": lambda tensor: tensor.clone(memory_format=torch.contiguous_format),
}

# èŠ‚ç‚¹å°¾å‡½æ•°ï¼ˆè¾¹è¾“å‡ºåçš„å¼ é‡èšåˆï¼‰
# Node tail functions (tensor aggregation after edge output)
MHD_NODE_TAIL_FUNCS: Dict[str, Callable[..., Any]] = {
    "sum": lambda tensors: sum(tensors),
    "avg": lambda tensors: torch.stack(tensors).mean(dim=0),
    "max": lambda tensors: torch.stack(tensors).max(dim=0)[0],
    "min": lambda tensors: torch.stack(tensors).min(dim=0)[0],
    "mul": lambda tensors: torch.prod(torch.stack(tensors), dim=0)
}

# å·¥å…·å‡½æ•° (Utility Functions)
def MHD_sort_nodes_by_topo_attr(attrs: List[TopoAttr]) -> List[Tuple[int, int]]:
    """æŒ‰æ‹“æ‰‘å±æ€§çš„sortå­—æ®µæ’åºèŠ‚ç‚¹ (Sort nodes by sort field of topology attributes)"""
    indexed_nodes = list(enumerate(attrs))
    return sorted(indexed_nodes, key=lambda p: p[1].sort if p[1] is not None else 0)

def MHD_flatten_tensor(x: torch.Tensor) -> torch.Tensor:
    """å±•å¹³å¼ é‡ï¼ˆä¿ç•™æ‰¹æ¬¡ç»´åº¦ï¼‰ (Flatten tensor (keep batch dimension))"""
    if x.dim() > 2:
        x_flat = x.reshape(x.shape[0], -1)
    else:
        x_flat = x
    return x_flat

def extract_operation_name(op: Union[str, nn.Module]) -> str:
    """æå–æ“ä½œåç§°ï¼ˆç§»é™¤è·¯å¾„/å‚æ•°ï¼‰ (Extract operation name (remove path/parameters))"""
    if isinstance(op, nn.Module):
        op_name = op.__class__.__name__
    elif isinstance(op, str):
        op_name = re.sub(r'\(.*\)', '', op).strip()
    else:
        op_name = str(op)

    op_name = op_name.replace("torch.nn.modules.", "").replace("torch.nn.", "")
    return op_name

def parse_topo_value(value: Union[dict, TopoAttr]) -> TopoAttr:
    """è§£ææ‹“æ‰‘å€¼ä¸ºç»“æ„åŒ–å±æ€§ (Parse topology value to structured attributes)"""
    if isinstance(value, dict):
        return TopoAttr(
            role=value.get("role", 0),
            sort=value.get("sort", 0),
            ext=value.get("ext", {})
        )
    elif isinstance(value, TopoAttr):
        return value
    else:
        raise ValueError(f"ä»…æ”¯æŒdict/TopoAttræ ¼å¼ (Only dict/TopoAttr format supported)ï¼ŒGot {type(value)}")

# æ ¸å¿ƒæ“ä½œå‡½æ•° (Core Operation Functions)
def MHD_concat(tensors: List[torch.Tensor], attrs: List[TopoAttr]) -> torch.Tensor:
    """æŒ‰æ‹“æ‰‘å±æ€§æ’åºåæ‹¼æ¥å¼ é‡ (Concatenate tensors after sorting by topology attributes)"""
    sorted_pairs = MHD_sort_nodes_by_topo_attr(attrs)
    sorted_tensors = [tensors[i] for i, _ in sorted_pairs]
    return torch.cat(sorted_tensors, dim=1)

def MHD_matmul(tensors: List[torch.Tensor], attrs: List[TopoAttr]) -> torch.Tensor:
    """æŒ‰æ‹“æ‰‘å±æ€§æ’åºåæ‰§è¡ŒçŸ©é˜µä¹˜æ³• (Perform matrix multiplication after sorting by topology attributes)"""
    sorted_pairs = MHD_sort_nodes_by_topo_attr(attrs)
    sorted_tensors = [tensors[i] for i, _ in sorted_pairs]
    if len(sorted_tensors) != 2:
        raise ValueError(f"Matmuléœ€è¦2ä¸ªè¾“å…¥å¼ é‡ (Matmul requires 2 input tensors)ï¼ŒGot {len(sorted_tensors)}")
    return torch.matmul(*sorted_tensors)

MHD_EDGE_IN_FUNCS: Dict[str, Callable[..., Any]] = {
    "concat": MHD_concat,
    "matmul": MHD_matmul,
}

def MHD_split(x: torch.Tensor, attrs: List[TopoAttr], node_channels: List[int]) -> List[torch.Tensor]:
    """æŒ‰æ‹“æ‰‘å±æ€§åˆ†å‰²å¼ é‡ (Split tensor by topology attributes)"""
    sorted_nodes = MHD_sort_nodes_by_topo_attr(attrs)
    sorted_original_indices = [p[0] for p in sorted_nodes]
    sorted_channel_sizes = [node_channels[i] for i in sorted_original_indices]

    split_tensors = torch.split(x, sorted_channel_sizes, dim=1)
    tensor_map = {idx: t for idx, t in zip(sorted_original_indices, split_tensors)}
    return [tensor_map[i] for i in range(len(attrs))]

def MHD_svd(x: torch.Tensor, attrs: List[TopoAttr], node_channels: List[int]) -> List[torch.Tensor]:
    """æŒ‰æ‹“æ‰‘å±æ€§æ‰§è¡ŒSVDåˆ†è§£ (Perform SVD decomposition by topology attributes)"""
    x_flat = MHD_flatten_tensor(x)
    U, S, Vh = torch.linalg.svd(x_flat, full_matrices=False)

    sorted_nodes = MHD_sort_nodes_by_topo_attr(attrs)
    svd_components = [U, S, Vh]
    sorted_tensors = []

    for i, (orig_idx, _) in enumerate(sorted_nodes):
        comp_idx = i % len(svd_components)
        tensor = svd_components[comp_idx]
        sorted_tensors.append((orig_idx, tensor))

    tensor_map = {idx: t for idx, t in sorted_tensors}
    return [tensor_map[i] for i in range(len(attrs))]

def MHD_lu(x: torch.Tensor, attrs: List[TopoAttr], node_channels: List[int]) -> List[torch.Tensor]:
    """æŒ‰æ‹“æ‰‘å±æ€§æ‰§è¡ŒLUåˆ†è§£ (Perform LU decomposition by topology attributes)"""
    x_flat = MHD_flatten_tensor(x)
    P, L, U = torch.linalg.lu(x_flat)

    sorted_nodes = MHD_sort_nodes_by_topo_attr(attrs)
    lu_components = [L, U, P]
    sorted_tensors = []

    for i, (orig_idx, _) in enumerate(sorted_nodes):
        comp_idx = i % len(lu_components)
        tensor = lu_components[comp_idx]
        sorted_tensors.append((orig_idx, tensor))

    tensor_map = {idx: t for idx, t in sorted_tensors}
    return [tensor_map[i] for i in range(len(attrs))]

MHD_EDGE_OUT_FUNCS: Dict[str, Callable[..., Any]] = {
    "split": MHD_split,
    "svd": MHD_svd,
    "lu": MHD_lu,
}

# å­—ç¬¦ä¸²æ“ä½œåŒ…è£…ç±» (String Operation Wrapper Class)
class StringOperation(nn.Module):
    """å­—ç¬¦ä¸²å®šä¹‰çš„å¼ é‡æ“ä½œåŒ…è£…ç±» (Wrapper class for tensor operations defined by string)"""
    def __init__(self, op_str: str):
        super().__init__()
        self.op_str = op_str

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if '(' in self.op_str and ')' in self.op_str:
            method_name, args_str = self.op_str.split('(', 1)
            args_str = args_str.rstrip(')')
            args = []
            kwargs = {}

            if args_str:
                for arg in args_str.split(','):
                    arg = arg.strip()
                    if not arg:
                        continue
                    if '=' in arg:
                        k, v = arg.split('=', 1)
                        kwargs[k.strip()] = eval(v.strip())
                    else:
                        args.append(eval(arg.strip()))

            return getattr(x, method_name)(*args, **kwargs)
        else:
            return getattr(x, self.op_str)()

# æ ¸å¿ƒæ•°æ®ç»“æ„ (Core Data Structures)
@dataclass
class MHD_Node:
    """è¶…å›¾èŠ‚ç‚¹ç±» (Hypergraph Node Class)"""
    id: int
    name: str
    value: torch.Tensor
    func: Dict[str, str] = field(default_factory=lambda: {"head": "share", "tail": "sum"})

    def __hash__(self):
        return hash((self.id, self.name))

    def __eq__(self, other):
        if not isinstance(other, MHD_Node):
            return False
        return self.id == other.id and self.name == other.name

@dataclass
class MHD_Edge:
    """è¶…å›¾è¾¹ç±» (Hypergraph Edge Class)"""
    id: int
    name: str
    value: List[Union[str, nn.Module]]
    func: Dict[str, str] = field(default_factory=lambda: {"in": "concat", "out": "split"})

    def __hash__(self):
        return hash((self.id, self.name))

    def __eq__(self, other):
        if not isinstance(other, MHD_Edge):
            return False
        return self.id == other.id and self.name == other.name

@dataclass
class MHD_Topo:
    """è¶…å›¾æ‹“æ‰‘ç±» (Hypergraph Topology Class)"""
    value: List[List[Union[TopoAttr, dict]]]

    def __post_init__(self):
        self.value = [
            [parse_topo_value(val) for val in row]
            for row in self.value
        ]

    def get_topo_attr(self, edge_id: int, node_id: int) -> TopoAttr:
        """è·å–æŒ‡å®šè¾¹å’ŒèŠ‚ç‚¹çš„æ‹“æ‰‘å±æ€§ (Get topology attribute of specified edge and node)"""
        return self.value[edge_id][node_id]

    def to_tensor(self) -> torch.Tensor:
        """è½¬æ¢ä¸ºå¼ é‡å½¢å¼ (Convert to tensor form)"""
        tensor_data = []
        for row in self.value:
            tensor_row = [attr.role for attr in row]
            tensor_data.append(tensor_row)
        return torch.tensor(tensor_data, dtype=torch.int64)

# æ‹“æ‰‘æ’åº (Topological Sorting)
def MHD_topological_sort(nodes: set[MHD_Node], edges: set[MHD_Edge], topo: MHD_Topo) -> List[int]:
    """è¶…å›¾èŠ‚ç‚¹æ‹“æ‰‘æ’åº (Hypergraph node topological sorting)"""
    graph = defaultdict(list)
    in_degree = defaultdict(int)
    all_node_ids = {node.id for node in nodes}

    if len(topo.value) > 0 and len(topo.value[0]) > 0:
        for edge_id, edge_row in enumerate(topo.value):
            for node_id, attr in enumerate(edge_row):
                if attr.role == 0:
                    continue
                if attr.role < 0:
                    head_id = node_id
                    tail_ids = [
                        nid for nid, a in enumerate(edge_row) 
                        if a.role > 0
                    ]
                    for tail_id in tail_ids:
                        graph[head_id].append(tail_id)
                        in_degree[tail_id] += 1

    for node_id in all_node_ids:
        if node_id not in in_degree:
            in_degree[node_id] = 0

    queue = deque([node_id for node_id in all_node_ids if in_degree[node_id] == 0])
    sorted_node_ids = []

    while queue:
        current_node = queue.popleft()
        sorted_node_ids.append(current_node)
        for neighbor in graph[current_node]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)

    if len(sorted_node_ids) != len(all_node_ids):
        raise ValueError(f"è¶…å›¾æ£€æµ‹åˆ°ç¯ (Hypergraph detected cycle)ï¼ç¯ä¸­èŠ‚ç‚¹ (Nodes in cycle): {all_node_ids - set(sorted_node_ids)}")
    return sorted_node_ids

# åŠ¨æ€ç½‘ç»œ (Dynamic Network)
class DNet(nn.Module):
    """è¶…è¾¹æ“ä½œåŠ¨æ€ç½‘ç»œ (Dynamic network for hyperedge operations)"""
    def __init__(self, operations: List[Union[str, nn.Module]]):
        super().__init__()
        seq_ops = []
        self.op_names = []

        for op in operations:
            self.op_names.append(extract_operation_name(op))

            if isinstance(op, nn.Module):
                seq_ops.append(op)
            elif isinstance(op, str):
                seq_ops.append(StringOperation(op))
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹ (Unsupported operation type): {type(op)}ï¼Œä»…æ”¯æŒnn.Module/string (only nn.Module/string supported)")

        self.filter = nn.Sequential(*seq_ops)
        self.original_operations = operations

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.filter(x)

# è¶…å›¾åŠ¨æ€ç½‘ç»œ (Hypergraph Dynamic Network)
class HDNet(nn.Module):
    """è¶…å›¾åŠ¨æ€ç½‘ç»œï¼ˆå•å­ç½‘ï¼‰ (Hypergraph dynamic network (single subnet))"""
    def __init__(self, nodes: set[MHD_Node], edges: set[MHD_Edge], topo: MHD_Topo):
        super().__init__()
        self.node_id2obj = {node.id: node for node in nodes}
        self.edge_id2obj = {edge.id: edge for edge in edges}
        self.topo = topo

        self._validate_topo()
        self.sorted_node_ids = MHD_topological_sort(nodes, edges, topo)
        print(f"âœ… æ‹“æ‰‘æ’åºå®Œæˆ (Topological sort completed): {[self.node_id2obj[nid].name for nid in self.sorted_node_ids]}")

        self.edge_nets = nn.ModuleDict()
        for edge in edges:
            self.edge_nets[edge.name] = DNet(edge.value)

        # èŠ‚ç‚¹å€¼åˆå§‹åŒ– (Node value initialization)
        self.node_values = {node.id: node.value for node in nodes}

    @property
    def node_name2id(self):
        """èŠ‚ç‚¹åç§°åˆ°IDçš„æ˜ å°„ (Node name to ID mapping)"""
        return {v.name: k for k, v in self.node_id2obj.items()}

    def _validate_topo(self) -> None:
        """éªŒè¯æ‹“æ‰‘çŸ©é˜µç»´åº¦ (Validate topology matrix dimensions)"""
        num_edges = len(self.edge_id2obj)
        num_nodes = len(self.node_id2obj)

        if len(self.topo.value) != num_edges:
            raise ValueError(
                f"æ‹“æ‰‘çŸ©é˜µè¾¹ç»´åº¦ä¸åŒ¹é… (Topology matrix edge dimension mismatch): é¢„æœŸ (expected){num_edges}ï¼Œå®é™… (actual){len(self.topo.value)}"
            )
        for edge_row in self.topo.value:
            if len(edge_row) != num_nodes:
                raise ValueError(
                    f"æ‹“æ‰‘çŸ©é˜µèŠ‚ç‚¹ç»´åº¦ä¸åŒ¹é… (Topology matrix node dimension mismatch): é¢„æœŸ (expected){num_nodes}ï¼Œå®é™… (actual){len(edge_row)}"
                )

        for edge_id, edge_row in enumerate(self.topo.value):
            for node_id, attr in enumerate(edge_row):
                if not isinstance(attr, TopoAttr):
                    raise ValueError(
                        f"æ‹“æ‰‘çŸ©é˜µå…ƒç´  (Topology matrix element)({edge_id}, {node_id})å¿…é¡»ä¸ºTopoAttr (must be TopoAttr)ï¼ŒGot {type(attr)}"
                    )

    def get_node_by_name(self, name: str) -> MHD_Node:
        """æŒ‰åç§°è·å–èŠ‚ç‚¹ (Get node by name)"""
        try:
            node_id = self.node_name2id[name]
            return self.node_id2obj[node_id]
        except KeyError:
            raise ValueError(f"èŠ‚ç‚¹åç§°ä¸å­˜åœ¨ (Node name does not exist): {name}")

    def forward(self) -> Dict[str, Tensor]:
        """æ‹“æ‰‘é©±åŠ¨å‰å‘ä¼ æ’­ (Topology-driven forward propagation)"""
        device = next(iter(self.node_values.values())).device

        edge_affects_nodes = defaultdict(list)
        if len(self.topo.value) > 0:
            for edge_id in self.edge_id2obj.keys():
                edge_row = self.topo.value[edge_id]
                tail_node_ids = [nid for nid, attr in enumerate(edge_row) if attr.role > 0]
                edge_affects_nodes[edge_id] = tail_node_ids

        for target_node_id in self.sorted_node_ids:
            relevant_edges = [eid for eid, node_ids in edge_affects_nodes.items() if target_node_id in node_ids]

            for edge_id in relevant_edges:
                edge = self.edge_id2obj[edge_id]
                edge_net = self.edge_nets[edge.name]
                edge_row = self.topo.value[edge_id]

                # è·å–å¤´èŠ‚ç‚¹ (Get head nodes)
                head_mask = [attr.role < 0 for attr in edge_row]
                head_node_ids = [i for i, val in enumerate(head_mask) if val]
                head_topo_attrs = [edge_row[nid] for nid in head_node_ids]

                # å¤„ç†å¤´èŠ‚ç‚¹å¼ é‡ (Process head node tensors)
                head_tensors = []
                for node_id in head_node_ids:
                    node = self.node_id2obj[node_id]
                    head_func_name = node.func.get("head", "share")
                    head_tensor = MHD_NODE_HEAD_FUNCS[head_func_name](self.node_values[node_id])
                    head_tensors.append(head_tensor)

                # è¾¹è¾“å…¥å¤„ç† (Edge input processing)
                edge_in_func_name = edge.func.get("in", "concat")
                edge_input = MHD_EDGE_IN_FUNCS[edge_in_func_name](head_tensors, head_topo_attrs)

                # è¾¹æ“ä½œå‰å‘ä¼ æ’­ (Edge operation forward propagation)
                edge_output = edge_net(edge_input)

                # è·å–å°¾èŠ‚ç‚¹ (Get tail nodes)
                tail_mask = [attr.role > 0 for attr in edge_row]
                tail_node_ids = [i for i, val in enumerate(tail_mask) if val]
                tail_topo_attrs = [edge_row[nid] for nid in tail_node_ids]
                tail_node_channels = [self.node_id2obj[node_id].value.shape[1] for node_id in tail_node_ids]

                # è¾¹è¾“å‡ºå¤„ç† (Edge output processing)
                edge_out_func_name = edge.func.get("out", "split")
                tail_tensors = MHD_EDGE_OUT_FUNCS[edge_out_func_name](
                    edge_output, tail_topo_attrs, tail_node_channels
                )

                # èŠ‚ç‚¹å€¼æ›´æ–° (Node value update)
                for node_id, tensor in zip(tail_node_ids, tail_tensors):
                    node = self.node_id2obj[node_id]
                    tail_func_name = node.func.get("tail", "sum")
                    if node_id in self.node_values:
                        agg_tensor = MHD_NODE_TAIL_FUNCS[tail_func_name](
                            [self.node_values[node_id], tensor]
                        )
                        self.node_values[node_id] = agg_tensor
                    else:
                        self.node_values[node_id] = tensor

        return {
            self.node_id2obj[node_id].name: tensor
            for node_id, tensor in self.node_values.items()
        }

# å¤šè¶…å›¾åŠ¨æ€ç½‘ç»œ (Multi-Hypergraph Dynamic Network)
class MHDNet(HDNet):
    """å¤šè¶…å›¾åŠ¨æ€ç½‘ç»œï¼ˆå…¨å±€è¶…å›¾ï¼‰ (Multi-hypergraph dynamic network (global hypergraph))"""
    def __init__(
        self,
        hdnet_list: List[Tuple[str, HDNet]],
        node_group: Tuple[Set[str], ...],
    ):
        # 1. æ„å»ºå…¨å±€èŠ‚ç‚¹/è¾¹/æ‹“æ‰‘ (Build global nodes/edges/topology)
        global_nodes, global_edges, global_topo_data = self._build_global_hypergraph(hdnet_list, node_group)
        
        # 2. åˆå§‹åŒ–çˆ¶ç±»HDNet (Initialize parent class HDNet)
        global_topo = MHD_Topo(value=global_topo_data)
        super().__init__(nodes=global_nodes, edges=global_edges, topo=global_topo)
        
        # 3. ä¿å­˜åŸå§‹æ˜ å°„ (Save original mappings)
        self.hdnet_list = hdnet_list
        self.node_group = node_group

    def _build_global_hypergraph(self, hdnet_list: List[Tuple[str, HDNet]], node_group: Tuple[Set[str], ...]) -> Tuple[Set[MHD_Node], Set[MHD_Edge], List[List[TopoAttr]]]:
        """æ„å»ºå®Œæ•´çš„å…¨å±€è¶…å›¾ï¼ˆåŒ…å«æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹ï¼‰ (Build complete global hypergraph with all nodes and edges)"""
        # ===================== æ­¥éª¤1ï¼šé¢„å¤„ç†æ‰€æœ‰å­å›¾èŠ‚ç‚¹/è¾¹ =====================
        # å­èŠ‚ç‚¹æ˜ å°„ï¼škey=suffix::name, value=(suffix, sub_node_id, sub_node)
        sub_node_map = {}
        # å­è¾¹æ˜ å°„ï¼škey=suffix::name, value=(suffix, sub_edge_id, sub_edge)
        sub_edge_map = {}
        # æ‰€æœ‰å­èŠ‚ç‚¹åç§°é›†åˆï¼ˆç”¨äºåç»­å»é‡ï¼‰
        all_sub_node_names = set()
        
        for suffix, hdnet in hdnet_list:
            # å¤„ç†èŠ‚ç‚¹
            for sub_node_id, sub_node in hdnet.node_id2obj.items():
                global_node_name = f"{suffix}::{sub_node.name}"
                sub_node_map[global_node_name] = (suffix, sub_node_id, sub_node)
                all_sub_node_names.add(global_node_name)
            
            # å¤„ç†è¾¹ï¼ˆæ‰€æœ‰å­å›¾è¾¹éƒ½ä½œä¸ºç‹¬ç«‹å…¨å±€è¾¹ï¼‰
            for sub_edge_id, sub_edge in hdnet.edge_id2obj.items():
                global_edge_name = f"{suffix}::{sub_edge.name}"
                sub_edge_map[global_edge_name] = (suffix, sub_edge_id, sub_edge)

        # ===================== æ­¥éª¤2ï¼šå¤„ç†èŠ‚ç‚¹åˆå¹¶ =====================
        node_id_counter = 0
        merged_node_map = {}  # key=å…¨å±€èŠ‚ç‚¹å, value=MHD_Node
        sub2global_node = {}  # key=å­èŠ‚ç‚¹å(suffix::name), value=å…¨å±€èŠ‚ç‚¹å
        
        # ç¬¬ä¸€æ­¥ï¼šå…ˆå¤„ç†éœ€è¦åˆå¹¶çš„èŠ‚ç‚¹ç»„
        merged_node_names = set()
        for node_group in node_group:
            # æŒ‰hdnet_listé¡ºåºæ’åºèŠ‚ç‚¹ç»„
            sorted_node_names = sorted(
                node_group,
                key=lambda x: next((i for i, (suffix, _) in enumerate(hdnet_list) if x.startswith(f"{suffix}::")), 999)
            )
            merged_name = "-".join(sorted_node_names)
            merged_node_names.update(sorted_node_names)  # è®°å½•è¢«åˆå¹¶çš„å­èŠ‚ç‚¹
            
            # è·å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºåŸºç¡€å€¼
            first_node_name = sorted_node_names[0]
            _, _, base_node = sub_node_map[first_node_name]
            
            # åˆ›å»ºåˆå¹¶èŠ‚ç‚¹
            merged_node = MHD_Node(
                id=node_id_counter,
                name=merged_name,
                value=base_node.value,
                func=base_node.func
            )
            merged_node_map[merged_name] = merged_node
            node_id_counter += 1
            
            # å»ºç«‹å­èŠ‚ç‚¹åˆ°åˆå¹¶èŠ‚ç‚¹çš„æ˜ å°„
            for node_name in sorted_node_names:
                sub2global_node[node_name] = merged_name

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æœªè¢«åˆå¹¶çš„ç‹¬ç«‹èŠ‚ç‚¹
        unmerged_node_names = all_sub_node_names - merged_node_names
        for node_name in sorted(unmerged_node_names):
            _, _, sub_node = sub_node_map[node_name]
            
            # åˆ›å»ºç‹¬ç«‹å…¨å±€èŠ‚ç‚¹ï¼ˆåç§°ä¿æŒsuffix::nameï¼‰
            unmerged_node = MHD_Node(
                id=node_id_counter,
                name=node_name,
                value=sub_node.value,
                func=sub_node.func
            )
            merged_node_map[node_name] = unmerged_node
            sub2global_node[node_name] = node_name  # æ˜ å°„åˆ°è‡ªèº«
            node_id_counter += 1

        # ===================== æ­¥éª¤3ï¼šæ„å»ºå…¨å±€è¾¹ï¼ˆæ‰€æœ‰å­å›¾è¾¹ç‹¬ç«‹å­˜åœ¨ï¼‰ =====================
        edge_id_counter = 0
        merged_edge_map = {}
        
        for global_edge_name, (suffix, sub_edge_id, sub_edge) in sub_edge_map.items():
            # åˆ›å»ºç‹¬ç«‹å…¨å±€è¾¹ï¼ˆåç§°ä¿æŒsuffix::edge_nameï¼‰
            merged_edge = MHD_Edge(
                id=edge_id_counter,
                name=global_edge_name,
                value=sub_edge.value,
                func=sub_edge.func
            )
            merged_edge_map[global_edge_name] = merged_edge
            edge_id_counter += 1

        # ===================== æ­¥éª¤4ï¼šæ„å»ºå…¨å±€æ‹“æ‰‘çŸ©é˜µ =====================
        global_topo_data = []
        global_node_name2id = {name: node.id for name, node in merged_node_map.items()}
        
        for global_edge_name, (suffix, sub_edge_id, sub_edge) in sub_edge_map.items():
            # è·å–å¯¹åº”å­å›¾çš„æ‹“æ‰‘è¡Œ
            hdnet = next(h for s, h in hdnet_list if s == suffix)
            sub_topo_row = hdnet.topo.value[sub_edge_id]
            
            # åˆå§‹åŒ–å…¨å±€æ‹“æ‰‘è¡Œï¼ˆé•¿åº¦=å…¨å±€èŠ‚ç‚¹æ•°ï¼‰
            global_topo_row = [TopoAttr(role=0, sort=0, ext={}) for _ in range(len(merged_node_map))]
            
            # æ˜ å°„å­æ‹“æ‰‘åˆ°å…¨å±€æ‹“æ‰‘
            for sub_node_id, sub_attr in enumerate(sub_topo_row):
                if sub_attr.role == 0:
                    continue  # æ— æ‹“æ‰‘å…³ç³»çš„èŠ‚ç‚¹è·³è¿‡
                
                # è·å–å­èŠ‚ç‚¹çš„å…¨å±€åç§°
                sub_node_name = hdnet.node_id2obj[sub_node_id].name
                full_sub_node_name = f"{suffix}::{sub_node_name}"
                
                # æ‰¾åˆ°å¯¹åº”çš„å…¨å±€èŠ‚ç‚¹åç§°å’ŒID
                if full_sub_node_name in sub2global_node:
                    global_node_name = sub2global_node[full_sub_node_name]
                    global_node_id = global_node_name2id[global_node_name]
                    
                    # è½¬ç§»æ‹“æ‰‘å±æ€§åˆ°å…¨å±€èŠ‚ç‚¹
                    global_topo_row[global_node_id] = sub_attr
            
            global_topo_data.append(global_topo_row)

        # ===================== è½¬æ¢ä¸ºé›†åˆå¹¶è¿”å› =====================
        global_nodes = set(merged_node_map.values())
        global_edges = set(merged_edge_map.values())
        
        return global_nodes, global_edges, global_topo_data


def generate_mermaid(hdnet: HDNet) -> str:
    """ç”Ÿæˆå¸¦é¢œè‰²åŒºåˆ†çš„æç®€æ‹“æ‰‘å¯è§†åŒ–Mermaidä»£ç """
    mermaid = [
        "graph TD",
        "",
        "    %% æ ·å¼å®šä¹‰ï¼šèŠ‚ç‚¹å’Œè¾¹åŒºåˆ†é¢œè‰²",
        "    classDef nodeStyle fill:#fff7e6,stroke:#fa8c16,stroke-width:2px,rounded:1",
        "    classDef edgeStyle fill:#e6f7ff,stroke:#1890ff,stroke-width:2px,rounded:1",
        "",
    ]

    # 1. å…ˆç»™æ‰€æœ‰èŠ‚ç‚¹æ·»åŠ æ ·å¼
    for node_id, node in hdnet.node_id2obj.items():
        mermaid.append(f"    {node.name}:::nodeStyle")
    
    # 2. éå†è¾¹ï¼Œæ·»åŠ è¾¹æ ·å¼å’Œè¿æ¥å…³ç³»
    for edge_id, edge in hdnet.edge_id2obj.items():
        edge_name = edge.name
        edge_row = hdnet.topo.value[edge_id]
        
        # æ·»åŠ è¾¹æ ·å¼
        mermaid.append(f"    {edge_name}:::edgeStyle")
        
        # è·å–å¤´/å°¾èŠ‚ç‚¹åç§°
        head_node_ids = [nid for nid, attr in enumerate(edge_row) if attr.role < 0]
        tail_node_ids = [nid for nid, attr in enumerate(edge_row) if attr.role > 0]
        head_node_names = [hdnet.node_id2obj[nid].name for nid in head_node_ids]
        tail_node_names = [hdnet.node_id2obj[nid].name for nid in tail_node_ids]
        
        # ç”Ÿæˆè¿æ¥å…³ç³»
        for head_node in head_node_names:
            mermaid.append(f"    {head_node} --> {edge_name}")
        for tail_node in tail_node_names:
            mermaid.append(f"    {edge_name} --> {tail_node}")
        
        mermaid.append("")

    mermaid_code = "\n".join(mermaid)
    print(mermaid_code)
    return mermaid_code

# ç¤ºä¾‹ç”¨æ³• (Example Usage)
def example_mhdnet2():
    """MHDNetç¤ºä¾‹ï¼ˆè‡ªå®šä¹‰3å­å›¾æ‹“æ‰‘+MULèšåˆï¼‰ (MHDNet example with custom 3-subgraph topology + MUL aggregation)"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float32
    print(f"âœ… ä½¿ç”¨è®¾å¤‡ (Using device): {device}")

    # ===================== å­HDNet1 (A1â†’B1ã€A1â†’D1ï¼ŒD1ç”¨MULèšåˆ) =====================
    # Sub HDNet1 (A1â†’B1, A1â†’D1, D1 uses MUL aggregation)
    nodes_net1 = {
        MHD_Node(
            id=0, 
            name="A1", 
            value=torch.randn(1, 3, 8, 8, 8, device=device, dtype=dtype),  # 3é€šé“ (3 channels)
            func={"head": "share", "tail": "sum"}
        ),
        MHD_Node(
            id=1, 
            name="B1", 
            value=torch.randn(1, 2, 8, 8, 8, device=device, dtype=dtype),  # 2é€šé“ (2 channels)
            func={"head": "share", "tail": "sum"}
        ),
        MHD_Node(
            id=2, 
            name="D1", 
            value=torch.randn(1, 4, 8, 8, 8, device=device, dtype=dtype),  # 4é€šé“ (4 channels)
            func={"head": "share", "tail": "mul"}
        ),
    }
    # è¶…è¾¹1ï¼šA1â†’B1ï¼ˆçº¯Moduleåˆ—è¡¨ï¼‰ (Hyperedge 1: A1â†’B1 (pure Module list))
    edge1_net1 = [
        nn.Conv3d(3, 2, kernel_size=3, padding=1, bias=False).to(device),
        nn.BatchNorm3d(2).to(device),
        nn.ReLU(inplace=True)
    ]
    # è¶…è¾¹2ï¼šA1â†’D1ï¼ˆçº¯Moduleåˆ—è¡¨ï¼‰ (Hyperedge 2: A1â†’D1 (pure Module list))
    edge2_net1 = [
        nn.Conv3d(3, 4, kernel_size=1, padding=0, bias=True).to(device),
        nn.Sigmoid()
    ]
    edges_net1 = {
        MHD_Edge(
            id=0, 
            name="e1_A1_to_B1", 
            value=edge1_net1,
            func={"in": "concat", "out": "split"}
        ),
        MHD_Edge(
            id=1, 
            name="e2_A1_to_D1", 
            value=edge2_net1,
            func={"in": "concat", "out": "split"}
        )
    }
    # æ‹“æ‰‘çŸ©é˜µï¼š2æ¡è¶…è¾¹ Ã— 3ä¸ªèŠ‚ç‚¹ (Topology matrix: 2 hyperedges Ã— 3 nodes)
    topo_net1 = MHD_Topo(value=[
        [{"role": -1, "sort": 1}, {"role": 1, "sort": 1}, {"role": 0, "sort": 0}],  # è¶…è¾¹1ï¼šA1(å¤´)â†’B1(å°¾)
        [{"role": -1, "sort": 1}, {"role": 0, "sort": 0}, {"role": 1, "sort": 1}]   # è¶…è¾¹2ï¼šA1(å¤´)â†’D1(å°¾)
    ])
    hdnet1 = HDNet(nodes=nodes_net1, edges=edges_net1, topo=topo_net1)

    # ===================== å­HDNet2 (A2+B2æ‹¼æ¥â†’C2) =====================
    # Sub HDNet2 (A2+B2 concatâ†’C2)
    nodes_net2 = {
        MHD_Node(
            id=0, 
            name="A2", 
            value=torch.randn(1, 3, 8, 8, 8, device=device, dtype=dtype),  # ä¸A1åŒç»´åº¦ (Same dimension as A1)
            func={"head": "share", "tail": "sum"}
        ),
        MHD_Node(
            id=1, 
            name="B2", 
            value=torch.randn(1, 2, 8, 8, 8, device=device, dtype=dtype),  # ä¸B1åŒç»´åº¦ (Same dimension as B1)
            func={"head": "share", "tail": "sum"}
        ),
        MHD_Node(
            id=2, 
            name="C2", 
            value=torch.randn(1, 5, 8, 8, 8, device=device, dtype=dtype),  # 5é€šé“ (5 channels)
            func={"head": "share", "tail": "sum"}
        ),
    }
    # è¶…è¾¹ï¼šA2+B2æ‹¼æ¥â†’C2 (Hyperedge: A2+B2 concatâ†’C2)
    edge1_net2 = [
        nn.Conv3d(5, 5, kernel_size=3, padding=1, groups=5, bias=False).to(device),  # åˆ†ç»„å·ç§¯ (Group convolution)
        nn.GELU(),
        nn.Conv3d(5, 5, kernel_size=1, padding=0, bias=True).to(device)             # 1x1è°ƒæ•´ (1x1 adjustment)
    ]
    edges_net2 = {
        MHD_Edge(
            id=0, 
            name="e1_A2B2_to_C2", 
            value=edge1_net2,
            func={"in": "concat", "out": "split"}
        )
    }
    # æ‹“æ‰‘çŸ©é˜µï¼š1æ¡è¶…è¾¹ Ã— 3ä¸ªèŠ‚ç‚¹ (Topology matrix: 1 hyperedge Ã— 3 nodes)
    topo_net2 = MHD_Topo(value=[
        [{"role": -1, "sort": 1}, {"role": -1, "sort": 2}, {"role": 1, "sort": 1}]
    ])
    hdnet2 = HDNet(nodes=nodes_net2, edges=edges_net2, topo=topo_net2)

    # ===================== å­HDNet3 (C3â†’D3ï¼ŒD3ç”¨MULèšåˆ) =====================
    # Sub HDNet3 (C3â†’D3, D3 uses MUL aggregation)
    nodes_net3 = {
        MHD_Node(
            id=0, 
            name="C3", 
            value=torch.randn(1, 5, 8, 8, 8, device=device, dtype=dtype),  # ä¸C2åŒç»´åº¦ (Same dimension as C2)
            func={"head": "share", "tail": "sum"}
        ),
        MHD_Node(
            id=1, 
            name="D3", 
            value=torch.randn(1, 4, 8, 8, 8, device=device, dtype=dtype),  # ä¸D1åŒç»´åº¦ (Same dimension as D1)
            func={"head": "share", "tail": "mul"}
        ),
    }
    # è¶…è¾¹ï¼šC3â†’D3ï¼ˆModule+å­—ç¬¦ä¸²æ“ä½œæ··åˆï¼‰ (Hyperedge: C3â†’D3 (mixed Module+string operations))
    edge1_net3 = [
        nn.Conv3d(5, 4, kernel_size=3, padding=1, bias=False).to(device),
        nn.Softplus(),
        '__mul__(0.5)'  # å­—ç¬¦ä¸²æ“ä½œ (String operation)
    ]
    edges_net3 = {
        MHD_Edge(
            id=0, 
            name="e1_C3_to_D3", 
            value=edge1_net3,
            func={"in": "concat", "out": "split"}
        )
    }
    # æ‹“æ‰‘çŸ©é˜µï¼š1æ¡è¶…è¾¹ Ã— 2ä¸ªèŠ‚ç‚¹ (Topology matrix: 1 hyperedge Ã— 2 nodes)
    topo_net3 = MHD_Topo(value=[
        [{"role": -1, "sort": 1}, {"role": 1, "sort": 1}]
    ])
    hdnet3 = HDNet(nodes=nodes_net3, edges=edges_net3, topo=topo_net3)

    # ===================== æ„å»ºå…¨å±€HDNet =====================
    # Build global HDNet
    # 1. å®šä¹‰HDNetåˆ—è¡¨ (Define HDNet list)
    hdnet_list = [
        ("net1", hdnet1),
        ("net2", hdnet2),
        ("net3test", hdnet3)
    ]

    # 2. å®šä¹‰NodeFï¼ˆèŠ‚ç‚¹å®šä¹‰ç»„ï¼‰ (Define node_group (node definition groups))
    node_group = (
        {"net1::A1", "net2::A2"},          # åˆå¹¶ä¸º "net1::A1-net2::A2" (Merge to "net1::A1-net2::A2")
        {"net1::B1", "net2::B2"},          # åˆå¹¶ä¸º "net1::B1-net2::B2" (Merge to "net1::B1-net2::B2")
        {"net2::C2", "net3test::C3"},      # åˆå¹¶ä¸º "net2::C2-net3test::C3" (Merge to "net2::C2-net3test::C3")
        {"net1::D1", "net3test::D3"},      # åˆå¹¶ä¸º "net1::D1-net3test::D3" (Merge to "net1::D1-net3test::D3")
    )

    # 3. åˆ›å»ºMHDNetï¼ˆå…¨å±€è¶…å›¾ï¼‰ (Create MHDNet (global hypergraph))
    mhdnet = MHDNet(
        hdnet_list=hdnet_list,
        node_group=node_group
    )

    # ===================== åŸç”Ÿæ–¹å¼åŠ è½½æ•°æ® =====================
    # Load data with native operations
    # 1. å‡†å¤‡è¾“å…¥æ•°æ® (Prepare input data)
    input_tensor = torch.randn(1, 3, 8, 8, 8, device=device, dtype=dtype)
    print(f"\nâœ… è¾“å…¥å¼ é‡å½¢çŠ¶ (Input Tensor Shape): {input_tensor.shape}")

    # 2. åŸç”Ÿæ–¹å¼è®¿é—®å¹¶ä¿®æ”¹èŠ‚ç‚¹ (Directly access and modify node with native operations)
    # æŒ‰åç§°æŸ¥æ‰¾ç›®æ ‡èŠ‚ç‚¹ (Find target node by name)
    target_node_name = "net1::A1-net2::A2"
    input_node = mhdnet.get_node_by_name(target_node_name)
    
    # ä¿®æ”¹èŠ‚ç‚¹åç§°å’Œå€¼ (Modify node name and value)
    input_node.name = "input"  # é‡å‘½åä¸ºinput (Rename to input)
    input_node.value = input_tensor  # æ›´æ–°å€¼ (Update value)
    print(f"\nâœ… æ›´æ–°åçš„èŠ‚ç‚¹ (Updated Node) '{input_node.name}':")
    print(f"   - æ–°å€¼å½¢çŠ¶ (New Value Shape): {input_node.value.shape}")
    print(f"   - æ–°å€¼å‡å€¼ (New Value Mean): {input_node.value.mean().item():.4f}")

    # ===================== ç”Ÿæˆæœ€æ–°å¯è§†åŒ– =====================
    # Generate latest visualization
    print("MHD node_group - æœ€æ–°æ‹“æ‰‘å¯è§†åŒ– (Latest Topology Visualization):")
    print("="*80)
    generate_mermaid(mhdnet)
    print("="*80 + "\n")

    # ===================== è¿è¡Œç½‘ç»œ =====================
    # Run network
    print("ğŸš€ æ‰§è¡ŒMHDNetå‰å‘ä¼ æ’­ (Running MHDNet Forward Pass)...")
    all_features = mhdnet.forward()

    # æ‰“å°å…³é”®ç»“æœ (Print key results)
    print("\nâœ… è‡ªå®šä¹‰æ‹“æ‰‘å‰å‘ä¼ æ’­å®Œæˆ (Custom topology forward propagation completed)ï¼")
    print("\n=== å…¨å±€èŠ‚ç‚¹ç‰¹å¾è¯¦æƒ… (Global Node Feature Details) ===")
    for node_name in ["input", "net1::B1-net2::B2", "net2::C2-net3test::C3", "net1::D1-net3test::D3"]:
        if node_name in all_features:
            tensor = all_features[node_name]
            print(f"  - å…¨å±€èŠ‚ç‚¹ (Global Node) {node_name}: å½¢çŠ¶ (shape)={tensor.shape}, è®¾å¤‡ (device)={tensor.device}, å‡å€¼ (mean)={tensor.mean().item():.4f}")
    
    print(f"\nå…¨å±€èŠ‚ç‚¹æ€»æ•° (Total global nodes): {len(all_features)}")
    total_params = sum(p.numel() for p in mhdnet.parameters())
    print(f"æ¨¡å‹æ€»å‚æ•° (Total model parameters): {total_params:,}")
    
    return mhdnet

# æ¢¯åº¦éªŒè¯å‡½æ•° (Gradient Verification Function)
def verify_gradient(model):
    """éªŒè¯æ¢¯åº¦åä¼  (Verify gradient backpropagation)"""
    all_features = model.forward()
    output_tensor = all_features["net1::D1-net3test::D3"]
    loss = output_tensor.sum()
    
    model.zero_grad()
    loss.backward()
    
    has_gradient = False
    for name, param in model.named_parameters():
        if param.grad is not None and param.grad.sum() != 0:
            has_gradient = True
            print(f"âœ… å‚æ•° (Parameter) {name} æ¢¯åº¦æ­£å¸¸ (gradient normal): {param.grad.sum().item():.4f}")
    
    if has_gradient:
        print("\nâœ… æ¢¯åº¦åä¼ éªŒè¯é€šè¿‡ (Gradient backpropagation verification passed)ï¼")
    else:
        print("\nâŒ æ¢¯åº¦åä¼ éªŒè¯å¤±è´¥ (Gradient backpropagation verification failed)ï¼")

# ä¸»æ‰§è¡Œ (Main Execution)
if __name__ == "__main__":
    model = example_mhdnet2()
    verify_gradient(model)
